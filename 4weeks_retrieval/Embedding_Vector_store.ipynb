{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌐 텍스트 임베딩 모델\n",
    "\n",
    "### ℹ️ 정보\n",
    "텍스트 임베딩 모델 제공자와의 내장 통합에 대한 문서는 통합 섹션에서 확인하세요.\n",
    "\n",
    "`Embeddings` 클래스는 텍스트 임베딩 모델과의 인터페이스를 위해 설계된 클래스입니다. 많은 임베딩 모델 제공자들(OpenAI, Cohere, Hugging Face 등)이 있으며, 이 클래스는 모든 제공자들에 대해 표준 인터페이스를 제공하도록 설계되었습니다.\n",
    "\n",
    "임베딩은 텍스트의 벡터 표현을 생성합니다. 이는 텍스트를 벡터 공간에서 생각할 수 있게 하며, 벡터 공간에서 가장 유사한 텍스트 조각을 찾는 의미적 검색과 같은 작업을 할 수 있다는 것을 의미합니다.\n",
    "\n",
    "LangChain에서 기본 `Embeddings` 클래스는 두 가지 메소드를 제공합니다: 하나는 문서를 임베딩하기 위한 것이고, 다른 하나는 쿼리를 임베딩하기 위한 것입니다. 전자는 여러 텍스트를 입력으로 받으며, 후자는 단일 텍스트를 받습니다. 이를 두 가지 별도의 메소드로 제공하는 이유는 일부 임베딩 제공자들이 문서(검색 대상)와 쿼리(검색 쿼리 자체)에 대해 서로 다른 임베딩 방법을 가지고 있기 때문입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메시지를 리스트 형태에서 임베딩 벡터로 변환\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#쿼리 임베딩\n",
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "embedded_query[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 캐시백엔드임베딩 : 임베딩값을 저장시켜 같은값이 들어오면 추가적으로 실행하지 않고, 저장된 벡터에서 값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "underlying_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "store = LocalFileStore(\"./cache/\") #캐쉬파일로 저장을 하면\n",
    "\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings, store, namespace=underlying_embeddings.model\n",
    ")\n",
    "\n",
    "# store 를 토대로 임베딩 캐쉬를 통해 같은 내용이 들어오면 캐쉬된 값을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'your api_key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/kdb/Desktop/psedo_labs/4weeks_retrieval/Embedding_Vector_store.ipynb 셀 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kdb/Desktop/psedo_labs/4weeks_retrieval/Embedding_Vector_store.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_community\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocument_loaders\u001b[39;00m \u001b[39mimport\u001b[39;00m TextLoader\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/psedo_labs/4weeks_retrieval/Embedding_Vector_store.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_openai\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/psedo_labs/4weeks_retrieval/Embedding_Vector_store.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_text_splitters\u001b[39;00m \u001b[39mimport\u001b[39;00m CharacterTextSplitter\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "raw_documents = TextLoader('/Users/kdb/Desktop/psedo_labs/4weeks_retrieval/data/datas.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "\n",
    "# Document 로드하고 text spliter로 나누고 임베딩을 하고 vector store에 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\" #쿼리를 넣어서 유사도 검색\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = OpenAIEmbeddings().embed_query(query) #쿼리를 벡터로 변환하여 보다 정확한 검색\n",
    "docs = db.similarity_search_by_vector(embedding_vector)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "\n",
    "db = await Qdrant.afrom_documents(documents, embeddings, \"http://localhost:6333\") #비동기로 동작\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\" \n",
    "docs = await db.asimilarity_search(query)\n",
    "print(docs[0].page_content)\n",
    "\n",
    "embedding_vector = embeddings.embed_query(query)\n",
    "docs = await db.asimilarity_search_by_vector(embedding_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum marginal relevance search (MMR)\n",
    "- 최대 한계 관련성은 쿼리와의 유사성 및 선택한 문서 간의 다양성을 최적화합니다. 비동기 API에서도 지원됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "found_docs = await qdrant.amax_marginal_relevance_search(query, k=2, fetch_k=10) #최대 한계 관련성\n",
    "for i, doc in enumerate(found_docs):\n",
    "    print(f\"{i + 1}.\", doc.page_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \n",
    "    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]: # 쿼리를 받고, *는 키워드 인자를 받는다는 의미, run_manager는 콜백 매니저 \n",
    "        return [Document(page_content=query)] # 결과를 반환\n",
    "\n",
    "retriever = CustomRetriever()\n",
    "\n",
    "retriever.get_relevant_documents(\"bar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

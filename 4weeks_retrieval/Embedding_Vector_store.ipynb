{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸\n",
    "\n",
    "### â„¹ï¸ ì •ë³´\n",
    "í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸ ì œê³µìì™€ì˜ ë‚´ì¥ í†µí•©ì— ëŒ€í•œ ë¬¸ì„œëŠ” í†µí•© ì„¹ì…˜ì—ì„œ í™•ì¸í•˜ì„¸ìš”.\n",
    "\n",
    "`Embeddings` í´ë˜ìŠ¤ëŠ” í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸ê³¼ì˜ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ„í•´ ì„¤ê³„ëœ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ë§ì€ ì„ë² ë”© ëª¨ë¸ ì œê³µìë“¤(OpenAI, Cohere, Hugging Face ë“±)ì´ ìˆìœ¼ë©°, ì´ í´ë˜ìŠ¤ëŠ” ëª¨ë“  ì œê³µìë“¤ì— ëŒ€í•´ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ì˜ ë²¡í„° í‘œí˜„ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë²¡í„° ê³µê°„ì—ì„œ ìƒê°í•  ìˆ˜ ìˆê²Œ í•˜ë©°, ë²¡í„° ê³µê°„ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ì¡°ê°ì„ ì°¾ëŠ” ì˜ë¯¸ì  ê²€ìƒ‰ê³¼ ê°™ì€ ì‘ì—…ì„ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "LangChainì—ì„œ ê¸°ë³¸ `Embeddings` í´ë˜ìŠ¤ëŠ” ë‘ ê°€ì§€ ë©”ì†Œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤: í•˜ë‚˜ëŠ” ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ê¸° ìœ„í•œ ê²ƒì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ì¿¼ë¦¬ë¥¼ ì„ë² ë”©í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤. ì „ìëŠ” ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìœ¼ë©°, í›„ìëŠ” ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ë°›ìŠµë‹ˆë‹¤. ì´ë¥¼ ë‘ ê°€ì§€ ë³„ë„ì˜ ë©”ì†Œë“œë¡œ ì œê³µí•˜ëŠ” ì´ìœ ëŠ” ì¼ë¶€ ì„ë² ë”© ì œê³µìë“¤ì´ ë¬¸ì„œ(ê²€ìƒ‰ ëŒ€ìƒ)ì™€ ì¿¼ë¦¬(ê²€ìƒ‰ ì¿¼ë¦¬ ìì²´)ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ì„ë² ë”© ë°©ë²•ì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€ë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—ì„œ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì¿¼ë¦¬ ì„ë² ë”©\n",
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "embedded_query[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ìºì‹œë°±ì—”ë“œì„ë² ë”© : ì„ë² ë”©ê°’ì„ ì €ì¥ì‹œì¼œ ê°™ì€ê°’ì´ ë“¤ì–´ì˜¤ë©´ ì¶”ê°€ì ìœ¼ë¡œ ì‹¤í–‰í•˜ì§€ ì•Šê³ , ì €ì¥ëœ ë²¡í„°ì—ì„œ ê°’ì„ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "underlying_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "store = LocalFileStore(\"./cache/\") #ìºì‰¬íŒŒì¼ë¡œ ì €ì¥ì„ í•˜ë©´\n",
    "\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings, store, namespace=underlying_embeddings.model\n",
    ")\n",
    "\n",
    "# store ë¥¼ í† ëŒ€ë¡œ ì„ë² ë”© ìºì‰¬ë¥¼ í†µí•´ ê°™ì€ ë‚´ìš©ì´ ë“¤ì–´ì˜¤ë©´ ìºì‰¬ëœ ê°’ì„ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'your api_key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/kdb/Desktop/psedo_labs/4weeks_retrieval/Embedding_Vector_store.ipynb ì…€ 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kdb/Desktop/psedo_labs/4weeks_retrieval/Embedding_Vector_store.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_community\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocument_loaders\u001b[39;00m \u001b[39mimport\u001b[39;00m TextLoader\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/psedo_labs/4weeks_retrieval/Embedding_Vector_store.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_openai\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/psedo_labs/4weeks_retrieval/Embedding_Vector_store.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_text_splitters\u001b[39;00m \u001b[39mimport\u001b[39;00m CharacterTextSplitter\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "raw_documents = TextLoader('/Users/kdb/Desktop/psedo_labs/4weeks_retrieval/data/datas.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "\n",
    "# Document ë¡œë“œí•˜ê³  text spliterë¡œ ë‚˜ëˆ„ê³  ì„ë² ë”©ì„ í•˜ê³  vector storeì— ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\" #ì¿¼ë¦¬ë¥¼ ë„£ì–´ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = OpenAIEmbeddings().embed_query(query) #ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ë³´ë‹¤ ì •í™•í•œ ê²€ìƒ‰\n",
    "docs = db.similarity_search_by_vector(embedding_vector)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "\n",
    "db = await Qdrant.afrom_documents(documents, embeddings, \"http://localhost:6333\") #ë¹„ë™ê¸°ë¡œ ë™ì‘\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\" \n",
    "docs = await db.asimilarity_search(query)\n",
    "print(docs[0].page_content)\n",
    "\n",
    "embedding_vector = embeddings.embed_query(query)\n",
    "docs = await db.asimilarity_search_by_vector(embedding_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum marginal relevance search (MMR)\n",
    "- ìµœëŒ€ í•œê³„ ê´€ë ¨ì„±ì€ ì¿¼ë¦¬ì™€ì˜ ìœ ì‚¬ì„± ë° ì„ íƒí•œ ë¬¸ì„œ ê°„ì˜ ë‹¤ì–‘ì„±ì„ ìµœì í™”í•©ë‹ˆë‹¤. ë¹„ë™ê¸° APIì—ì„œë„ ì§€ì›ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "found_docs = await qdrant.amax_marginal_relevance_search(query, k=2, fetch_k=10) #ìµœëŒ€ í•œê³„ ê´€ë ¨ì„±\n",
    "for i, doc in enumerate(found_docs):\n",
    "    print(f\"{i + 1}.\", doc.page_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \n",
    "    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]: # ì¿¼ë¦¬ë¥¼ ë°›ê³ , *ëŠ” í‚¤ì›Œë“œ ì¸ìë¥¼ ë°›ëŠ”ë‹¤ëŠ” ì˜ë¯¸, run_managerëŠ” ì½œë°± ë§¤ë‹ˆì € \n",
    "        return [Document(page_content=query)] # ê²°ê³¼ë¥¼ ë°˜í™˜\n",
    "\n",
    "retriever = CustomRetriever()\n",
    "\n",
    "retriever.get_relevant_documents(\"bar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

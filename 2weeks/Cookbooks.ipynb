{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt + LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "api_key = 'your-api-key'\n",
    "prompt = ChatPromptTemplate.from_template(\"{country} ì˜ ì¤‘ìš”í•œ ì—­ì‚¬ëŠ” ë¬´ì—‡ì´ ìˆë‚˜ìš”?\")\n",
    "model = ChatOpenAI(api_key=api_key)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='í•œêµ­ì˜ ì¤‘ìš”í•œ ì—­ì‚¬ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‚¬ê±´ì´ ìˆìŠµë‹ˆë‹¤:\\n\\n1. ì„ì§„ì™œë€(1592ë…„~1598ë…„): ì¼ë³¸ì˜ ë„ìš”í† ë¯¸ íˆë°ìš”ì‹œê°€ í•œêµ­ì„ ì¹¨ê³µí•˜ì—¬ ë°œìƒí•œ ì „ìŸìœ¼ë¡œ, í•œêµ­ì€ í° í”¼í•´ë¥¼ ì…ì—ˆì§€ë§Œ, ê²°êµ­ì—ëŠ” ìŠ¹ë¦¬í•˜ì—¬ ê·€í™”í•˜ì˜€ìŠµë‹ˆë‹¤.\\n\\n2. ëŒ€í•œì œêµ­ ì„ í¬(1897ë…„): ëŒ€í•œì œêµ­ì€ 1897ë…„ì— ì„ í¬ë˜ì–´ ë…ë¦½ì ì¸ êµ­ê°€ë¡œ ìŠ¹ê²©ë˜ì—ˆìŠµë‹ˆë‹¤.\\n\\n3. ì¼ì œ ê°•ì ê¸°(1910ë…„~1945ë…„): ì¼ë³¸ì€ 1910ë…„ì— í•œêµ­ì„ ì‹ë¯¼ì§€ë¡œ í†µì¹˜í•˜ì˜€ê³ , 35ë…„ì— ê±¸ì³ ê°•ì œë¡œ í•œêµ­ì„ í†µì œí•˜ì˜€ìŠµë‹ˆë‹¤.\\n\\n4. 3.1 ìš´ë™(1919ë…„): ëŒ€í•œë¯¼êµ­ ê´‘ì£¼ì—ì„œ ì‹œì‘ëœ ì´ ìš´ë™ì€ í•œêµ­ì¸ë“¤ì˜ ë…ë¦½ì„ ìœ„í•œ í•­ì¼ ìš´ë™ìœ¼ë¡œ, ì¼ë³¸ì— ëŒ€í•œ í•­ê±°ì™€ í•œêµ­ì˜ ë…ë¦½ì„ ìš”êµ¬í•˜ì˜€ìŠµë‹ˆë‹¤.\\n\\n5. í•œêµ­ì „ìŸ(1950ë…„~1953ë…„): í•œêµ­ì „ìŸì€ ë‚¨ë¶í•œ ê°„ì˜ ë¶„ë‹¨ ì´í›„, ë¶í•œì˜ ì¹¨ê³µìœ¼ë¡œ ì¸í•˜ì—¬ ë°œìƒí•œ ì „ìŸìœ¼ë¡œ, ë§ì€ í¬ìƒìë¥¼ ë‚¸ ì „ìŸì´ì—ˆìŠµë‹ˆë‹¤.\\n\\nì´ëŸ¬í•œ ì—­ì‚¬ì  ì‚¬ê±´ë“¤ì€ í•œêµ­ì˜ ì—­ì‚¬ì™€ ë¬¸í™”ì— í° ì˜í–¥ì„ ë¯¸ì³¤ìœ¼ë©°, í˜„ì¬ì˜ í•œêµ­ì„ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤.', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"country\": \"í•œêµ­\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¯¸êµ­ì˜ ì¤‘ìš”í•œ ì—­ì‚¬ì—ëŠ” ë‹¤ì–‘í•œ ì‚¬ê±´ê³¼ ì¸ë¬¼ë“¤ì´ í¬í•¨ë˜ì§€ë§Œ, ì¼ë¶€ ì¤‘ìš”í•œ ì‚¬ê±´ê³¼ ì¸ë¬¼ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. ë¯¸êµ­ì˜ ë…ë¦½ìš´ë™ê³¼ ë¯¸êµ­ í˜ëª…: 1775ë…„ë¶€í„° 1783ë…„ê¹Œì§€ ì§„í–‰ëœ ë¯¸êµ­ì˜ ë…ë¦½ì „ìŸìœ¼ë¡œ ì¸í•´ ë¯¸êµ­ì€ ì˜êµ­ìœ¼ë¡œë¶€í„° ë…ë¦½ì„ ì„ ì–¸í•˜ê³  ë¯¸êµ­ í•©ì¤‘êµ­ìœ¼ë¡œ íƒ„ìƒí•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. ì—ì´ë¸ŒëŸ¬í–„ ë§ì»¨: ë¯¸êµ­ì˜ 16ëŒ€ ëŒ€í†µë ¹ìœ¼ë¡œì„œ ë¯¸êµ­ ë‚´ì „ì˜ ì£¼ìš” ì¸ë¬¼ ì¤‘ í•˜ë‚˜ë¡œ ê¼½íˆë©°, ë…¸ì˜ˆ í•´ë°© ì„ ì–¸ê³¼ ë¯¸êµ­ ë‚´ì „ì˜ ìŠ¹ë¦¬ë¥¼ ì´ëˆ ì¸ë¬¼ë¡œì„œ ì¤‘ìš”í•œ ì—­ì‚¬ì  ìœ„ì¹˜ë¥¼ ì°¨ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. ë¯¼ê¶Œ ìš´ë™: 20ì„¸ê¸° ì¤‘ë°˜ ë¯¸êµ­ì—ì„œ ë²Œì–´ì§„ ë¯¼ê¶Œ ìš´ë™ì€ ì¸ì¢… ì°¨ë³„ê³¼ ì¸ì¢… ë¶„ë¦¬ ì •ì±…ì— ë°˜ë°œí•˜ì—¬ ë¯¸êµ­ì˜ ì¸ê¶Œê³¼ ë¯¼ì£¼ì£¼ì˜ë¥¼ ì¦ì§„ì‹œí‚¤ëŠ” ë° í° ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤.\n",
      "\n",
      "4. ì„¸ê³„ ëŒ€ì „ ë° ëƒ‰ì „: ë¯¸êµ­ì€ 20ì„¸ê¸°ì— ë‘ ì°¨ë¡€ì˜ ì„¸ê³„ ëŒ€ì „ê³¼ ëƒ‰ì „ ì‹œê¸°ë¥¼ ê²ªìœ¼ë©° êµ­ì œ ì •ì¹˜ì™€ ê²½ì œì— í° ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤.\n",
      "\n",
      "5. 9/11 í…ŒëŸ¬: 2001ë…„ 9ì›” 11ì¼ ë°œìƒí•œ ë¯¸êµ­ ë‰´ìš•ì˜ ä¸–ç•Œè²¿æ˜“ä¸­å¿ƒì„ í…ŒëŸ¬ ê³µê²©ìœ¼ë¡œ íŒŒê´´ëœ ì‚¬ê±´ì€ ë¯¸êµ­ê³¼ ì„¸ê³„ ì—­ì‚¬ì— í° ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì´ ì™¸ì—ë„ ë¯¸êµ­ì˜ ì—­ì‚¬ëŠ” ë‹¤ì–‘í•œ ì¤‘ìš”í•œ ì‚¬ê±´ê³¼ ì¸ë¬¼ë“¤ì´ ìˆìœ¼ë©°, ì´ë¥¼ í†µí•´ ë¯¸êµ­ì˜ ë°œì „ê³¼ ì„±ì¥ì„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "for i in chain.stream({\"country\": \"ë¯¸êµ­\"}):\n",
    "    print(i.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model.bind(stop=[\"\\n\"]) # ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë©ˆì¶°ë¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='í”„ë‘ìŠ¤ì˜ ì¤‘ìš”í•œ ì—­ì‚¬ ì¤‘ ì¼ë¶€ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"country\": \"France\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "chain = {\"country\":RunnablePassthrough()} |prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ë…ì¼ì˜ ì¤‘ìš”í•œ ì—­ì‚¬ ì¤‘ ì¼ë¶€ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n1. ë¡œë§ˆ ì œêµ­ì˜ ë¶•ê´´ ì´í›„ ë…ì¼ ì§€ì—­ì—ì„œ ë°œìƒí•œ ë¶„ì—´ê³¼ êµ°ì£¼ê¶Œ ì‹¸ì›€\\n2. ì°°ìŠ¤ ëŒ€ì œì˜ ì°°ìŠ¤ì˜ í° ì¬ì •ë ¬ë¡œ ì¸í•œ ì‹ë¯¼ ì§€ë°°ì˜ ì‹¤í˜„\\n3. 19ì„¸ê¸° ë§ë¶€í„° 20ì„¸ê¸° ì´ˆê¹Œì§€ì˜ ë…ì¼ í†µì¼ ìš´ë™\\n4. ì œ1ì°¨ ì„¸ê³„ ëŒ€ì „ì—ì„œì˜ íŒ¨ë°°ì™€ ë² ë¥´ì‚¬ìœ  ì¡°ì•½ ì²´ê²°\\n5. ë‚˜ì¹˜ ë…ì¼ì˜ ì„¸ê³„ ëŒ€ì „ ì°¸ì „ê³¼ ìœ ëŒ€ì¸ ëŒ€í•™ì‚´\\n6. ì œ2ì°¨ ì„¸ê³„ ëŒ€ì „ì—ì„œì˜ íŒ¨ë°°ì™€ ë™ë…ê³¼ ì„œë…ì˜ ë¶„ë¦¬\\n7. ë² ë¥¼ë¦°ì˜ ì„±ë²½ ë¶•ê´´ë¡œ ë…ì¼ í†µì¼ì˜ ì„±ê³µ\\n8. ë¶ì•… ì£¼ë³€ì˜ ë² ë¥¼ë¦° ëŸ¬ë¸Œí˜¸í”„ì™€ ë…ì¼ì˜ ì¬í†µì¼.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"ë…ì¼\") #Runnablepassthroughì— ì˜í•´ {\"country\":\"ë…ì¼\"}ë¡œ ë³€í™˜ë¨ , StrOutputParserì— ì˜í•´ ë¬¸ìì—´ë¡œ ë³€í™˜ë¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¦œRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gpt-researcher 0.0.8 requires langchain==0.0.335, but you have langchain 0.1.13 which is incompatible.\n",
      "gpt-researcher 0.0.8 requires openai~=1.3.3, but you have openai 1.12.0 which is incompatible.\n",
      "gpt-researcher 0.0.8 requires tavily-python==0.2.8, but you have tavily-python 0.3.1 which is incompatible.\n",
      "langchain-experimental 0.0.43 requires langchain<0.1,>=0.0.342, but you have langchain 0.1.13 which is incompatible.\n",
      "tavily-python 0.3.1 requires tiktoken==0.5.2, but you have tiktoken 0.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "korea = \"\"\"\n",
    "ëŒ€í•œë¯¼êµ­ ì¶•êµ¬ëŒ€í‘œíŒ€ ì„ ìˆ˜ë‹¨ ëª…ë‹¨\n",
    "---\n",
    "ê³µê²©ìˆ˜ (FW)\n",
    "ì •ìš°ì˜ (JEONG Wooyeong): 1999.09.20, 180cm / 70kg, VfB ìŠˆíˆ¬íŠ¸ê°€ë¥´íŠ¸\n",
    "ì¡°ê·œì„± (CHO Guesung): 1998.01.25, 189cm / 82kg, FC ë¯¸íŠ¸ìœŒë€\n",
    "ì£¼ë¯¼ê·œ (JOO Minkyu): 1990.04.13, 183cm / 79kg, ìš¸ì‚°í˜„ëŒ€\n",
    "---\n",
    "ë¯¸ë“œí•„ë” (MF)\n",
    "ë°±ìŠ¹í˜¸ (PAIK Seungho): 1997.03.17, 182cm / 72kg, ë²„ë°ì—„ ì‹œí‹°\n",
    "ì†í¥ë¯¼ (SON Heungmin): 1992.07.08, 183cm / 78kg, í† íŠ¸ë„˜\n",
    "ì†¡ë¯¼ê·œ (SONG Minkyu): 1999.09.12, 179cm / 72kg, ì „ë¶í˜„ëŒ€ëª¨í„°ìŠ¤\n",
    "ì´ê°•ì¸ (LEE Kangin): 2001.02.19, 174cm / 72kg, íŒŒë¦¬ ìƒì œë¥´ë§¹\n",
    "ì´ì¬ì„± (LEE Jaesung): 1992.08.10, 180cm / 70kg, FSV ë§ˆì¸ì¸ 05\n",
    "ì •í˜¸ì—° (JEONG Hoyeon): 2000.09.28, 179cm / 68kg, ê´‘ì£¼FC\n",
    "í™í˜„ì„ (HONG Hyunseok): 1999.06.16, 175cm / 66kg, KAA í—¨íŠ¸\n",
    "í™©ì¸ë²” (HWANG Inbeom): 1996.09.20, 177cm / 70kg, FK ì¸ ë¥´ë² ë‚˜ ì¦ˆë² ì¦ˆë‹¤\n",
    "---\n",
    "ìˆ˜ë¹„ìˆ˜ (DF)\n",
    "ê¶Œê²½ì› (KWON Kyungwon): 1992.01.31, 188cm / 83kg, ìˆ˜ì›FC\n",
    "ê¹€ë¬¸í™˜ (KIM Moonhwan): 1995.08.01, 173cm / 64kg, ì•Œ ë‘í•˜ì¼ SC\n",
    "ê¹€ë¯¼ì¬ (KIM Minjae): 1996.11.15, 190cm / 88kg, FC ë°”ì´ì—ë¥¸ ë®Œí—¨\n",
    "ê¹€ì˜ê¶Œ (KIM Younggwon): 1990.02.27, 186cm / 83kg, ìš¸ì‚°í˜„ëŒ€\n",
    "ê¹€ì§„ìˆ˜ (KIM Jinsu): 1992.06.13, 177cm / 68kg, ì „ë¶í˜„ëŒ€ëª¨í„°ìŠ¤\n",
    "ë°•ì§„ì„­ (PARK Jinseop): 1995.10.23, 182cm / 75kg, ì „ë¶í˜„ëŒ€ëª¨í„°ìŠ¤\n",
    "ì„¤ì˜ìš° (SEOL Youngwoo): 1998.12.05, 180cm / 72kg, ìš¸ì‚°í˜„ëŒ€\n",
    "ì´ëª…ì¬ (LEE Myungjae): 1993.11.04, 182cm / 68kg, ìš¸ì‚°í˜„ëŒ€\n",
    "ì¡°ìœ ë¯¼ (CHO Yumin): 1996.11.17, 182cm / 70kg, ìƒ¤ë¥´ì FC\n",
    "---\n",
    "ê³¨í‚¤í¼ (GK)\n",
    "ì†¡ë²”ê·¼ (SONG Bumkeun): 1997.10.15, 194cm / 88kg, ì‡¼ë‚œ ë°¸ë§ˆë ˆ\n",
    "ì´ì°½ê·¼ (LEE Changgeun): 1993.08.30, 186cm / 75kg, ëŒ€ì „í•˜ë‚˜ì‹œí‹°ì¦Œ\n",
    "ì¡°í˜„ìš° (JO Hyeonwoo): 1991.09.25, 189cm / 75kg, ìš¸ì‚°í˜„ëŒ€\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "embedding=OpenAIEmbeddings(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = LocalFileStore(\"./cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=OpenAIEmbeddings(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embedding, store, namespace=embedding.model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text-embedding-ada-002a0146fd6-d992-5270-a7d0-4e0789f91cd1',\n",
       " 'text-embedding-ada-002218bc9da-cee2-586a-bc37-d41780259536',\n",
       " 'text-embedding-ada-002be8d00b4-6909-593c-a802-63ccb0a79efd',\n",
       " 'text-embedding-ada-00254317ce7-ea08-5df2-acc4-7ba7f72aeace',\n",
       " 'text-embedding-ada-00219bae6aa-dc2c-5fa3-9ca5-6905aabf4a78',\n",
       " 'text-embedding-ada-002b73d6f25-0773-5348-b0ed-9dcd53e10def',\n",
       " 'text-embedding-ada-0029a715424-06e5-58cb-ba63-60733eda344b',\n",
       " 'text-embedding-ada-00210a59bed-6d11-58ff-a06a-da9f5ea03a86',\n",
       " 'text-embedding-ada-0026c88acb2-9591-56f3-af7d-b467885e6a55',\n",
       " 'text-embedding-ada-002a6f393b1-3e09-5c26-b59d-714111fccebc',\n",
       " 'text-embedding-ada-00253133f22-d535-58ad-b3cb-464ef56df475',\n",
       " 'text-embedding-ada-0020d6e3ba2-323c-5da0-9404-73365db9ce19',\n",
       " 'text-embedding-ada-0020a95c2da-3730-55d1-ad8a-a8a71f1cc9ec',\n",
       " 'text-embedding-ada-002c9219b7d-53b8-540b-be4a-dd8d12c32d48',\n",
       " 'text-embedding-ada-0026926a656-ab19-5a8f-a415-37842d4e70c3',\n",
       " 'text-embedding-ada-0021c243508-585f-5561-9f2f-a6289a1035eb',\n",
       " 'text-embedding-ada-0022f3abaae-2a0b-5798-bbc1-b71487ecb9e8',\n",
       " 'text-embedding-ada-002406003a5-f81c-574a-b887-d92035dd6125',\n",
       " 'text-embedding-ada-00227fd8b76-ba68-5c11-8b58-131c9673ec0c',\n",
       " 'text-embedding-ada-0026836d5d4-629b-5c6b-bc08-56e17603a4e5',\n",
       " 'text-embedding-ada-0021bd398de-c441-558d-8550-394a76af604d',\n",
       " 'text-embedding-ada-002db813d41-dec0-5372-9dff-92eb516f0aee',\n",
       " 'text-embedding-ada-0025b42b701-2c3f-5865-a195-fafa3583af35',\n",
       " 'text-embedding-ada-0027bc0c433-17fb-54e3-b3d6-2fd38dad142b',\n",
       " 'text-embedding-ada-002ac1fda5c-7511-5ab2-819d-309aea243af4',\n",
       " 'text-embedding-ada-002cd9c1ae8-5b38-5910-8670-65729b4316cf',\n",
       " 'text-embedding-ada-00297b12021-0f7d-5868-b8d4-2f417c3a60af',\n",
       " 'text-embedding-ada-002baca9eb7-f70c-5310-9f37-c9f4476dadfd',\n",
       " 'text-embedding-ada-002f32b2328-b2b9-5038-8f07-9932003c3f01',\n",
       " 'text-embedding-ada-002744e5fb4-2d34-5a2e-9e93-34d595884277',\n",
       " 'text-embedding-ada-0021645e19f-e3e2-5020-a7f1-4328a98b47e1',\n",
       " 'text-embedding-ada-002f3fefb2b-65ea-531c-9953-60cfa61c6b5e',\n",
       " 'text-embedding-ada-002bd5b105e-e7dd-59c5-975c-31804d6583e2',\n",
       " 'text-embedding-ada-0024a97f114-17fa-5a84-ad51-530cd14bfbc3',\n",
       " 'text-embedding-ada-002dc6da046-2e14-521d-bdc4-277aad581859',\n",
       " 'text-embedding-ada-00263f5e682-5fdd-5218-8d42-8a1e3470a121',\n",
       " 'text-embedding-ada-0029b68e547-52d6-5364-9ec2-1de26ed633cf',\n",
       " 'text-embedding-ada-0023df861e4-5e29-5a08-9b79-b37fedc78528',\n",
       " 'text-embedding-ada-002e54946a8-f68f-5138-a3a7-5eb7a14798be',\n",
       " 'text-embedding-ada-002c761a907-d8c5-53a3-9df5-f0440a898368',\n",
       " 'text-embedding-ada-00236931354-2ce8-585b-b281-3e780c1d2268',\n",
       " 'text-embedding-ada-00237264890-582a-5ae9-8153-46267d2ccdbc',\n",
       " 'text-embedding-ada-002f4ec1193-cc49-5aad-ae44-f56df9098a0f',\n",
       " 'text-embedding-ada-0022b8b88ac-2bfc-54c8-827a-064661d1c6eb',\n",
       " 'text-embedding-ada-002e87ff996-aa34-5b7c-86fe-23b316436854',\n",
       " 'text-embedding-ada-002fcdfa9df-eced-5a11-b7ae-e6e9c543d3c1',\n",
       " 'text-embedding-ada-002aeb4c7be-0f84-5850-b3c7-8597899fc0e1',\n",
       " 'text-embedding-ada-0029e1f1ae2-61ab-5e04-96b4-079ca4b6b03a',\n",
       " 'text-embedding-ada-002207e5b7e-3b22-5e5a-9a88-a0430b103290',\n",
       " 'text-embedding-ada-0023fd3e20c-8a5b-5b89-83ce-a384fe36a731',\n",
       " 'text-embedding-ada-002fc085d72-f60e-5ad7-85ca-09ba40bf48b6',\n",
       " 'text-embedding-ada-0027f71373c-15f1-5e20-affc-7babb0c417ad',\n",
       " 'text-embedding-ada-00228eb731f-f309-5a8f-97f3-89ec805a111a',\n",
       " 'text-embedding-ada-002dca4c838-a81e-5a49-81c0-bda39f0e7cbc',\n",
       " 'text-embedding-ada-002f26c4891-ae64-5c44-bb1b-aed5a62bd4b4',\n",
       " 'text-embedding-ada-002ac6066fc-9a05-5fdc-a089-eae23c50b88e',\n",
       " 'text-embedding-ada-002405fb687-d5f1-52cc-840c-10dbbb796d45',\n",
       " 'text-embedding-ada-0029c04929a-c9de-50ac-9f47-fef40924b5d2',\n",
       " 'text-embedding-ada-0029252f2c1-6663-578d-9b49-133ca482da17',\n",
       " 'text-embedding-ada-0026d97b225-dd0e-536e-90dd-8b67f21d99d5',\n",
       " 'text-embedding-ada-0023037f876-1e90-502c-94b9-cc8a78718d23',\n",
       " 'text-embedding-ada-0028052b59a-05a3-5490-9eaf-32546736ff36',\n",
       " 'text-embedding-ada-002b1980fd9-b9df-58cb-9bba-72e89b5ea240',\n",
       " 'text-embedding-ada-0027a3e09e3-f599-5cf0-ba29-7d31cfb3e941',\n",
       " 'text-embedding-ada-002149ad106-ba6c-5228-857e-2957844c2d14',\n",
       " 'text-embedding-ada-002c75c86f7-8891-5db4-a7ff-44963404a933',\n",
       " 'text-embedding-ada-0024cad1f50-03ee-5005-a6ae-3ba04d6bea17',\n",
       " 'text-embedding-ada-0027da2d787-a811-5e80-a22b-672292f14d4a',\n",
       " 'text-embedding-ada-002c9ac368d-25c3-5c37-9de5-9c15df8c68ae',\n",
       " 'text-embedding-ada-0027f0736d5-09e1-5597-a92b-23d31633f70a',\n",
       " 'text-embedding-ada-002228c3ce5-f1a8-56e6-8ad0-aa35594bd9e8',\n",
       " 'text-embedding-ada-002b13a6412-f440-59cf-a3e4-4a70425c0caf',\n",
       " 'text-embedding-ada-0025c5a1403-cae6-5d7e-b0b3-94e6e6d157a7',\n",
       " 'text-embedding-ada-002dea4ce61-615f-5491-b16a-225b61ceb8c8',\n",
       " 'text-embedding-ada-002a69e71a9-f76b-513b-ad2c-e541c26ad4e2',\n",
       " 'text-embedding-ada-0027d3a8651-949a-59b4-b58f-8769a0ada62d',\n",
       " 'text-embedding-ada-0025fb7a5ee-5c4c-522b-8989-50d17b1eca7b',\n",
       " 'text-embedding-ada-00242a42fe7-1588-5499-8783-9ee3d1d8d61a',\n",
       " 'text-embedding-ada-0022fec60df-5d1b-5dcc-975f-532aa10be537',\n",
       " 'text-embedding-ada-0022cc57764-8c6a-5bf0-99e6-961a3a3ff5d2',\n",
       " 'text-embedding-ada-0020b4bb044-e0a2-534b-bee4-edd79a099082',\n",
       " 'text-embedding-ada-00203e5c71d-d5b5-5d81-9e5e-5efbbdb5fcc0',\n",
       " 'text-embedding-ada-002b5ab1ad3-cfab-54a3-93f5-96f1b6a5afef',\n",
       " 'text-embedding-ada-00292f6ba77-9031-5faa-a19d-25864b524ff6',\n",
       " 'text-embedding-ada-002d258e3e2-3c2c-59c6-9252-68849a1297c6',\n",
       " 'text-embedding-ada-002e4cf3463-c397-54cc-ab93-cf6d78d2199c',\n",
       " 'text-embedding-ada-0021a41a3c2-6f20-53ee-b18a-5b67e5c9360e',\n",
       " 'text-embedding-ada-002344e9bf1-2874-5ed2-ab5d-ef4f7d98e25d',\n",
       " 'text-embedding-ada-002abe30fb5-e074-54af-93a4-5a04ea776540',\n",
       " 'text-embedding-ada-002cc38d42c-9741-5218-8ef5-fefbb22e08f5',\n",
       " 'text-embedding-ada-002812b86c1-8ebf-5483-95c6-c95cf2b52d12',\n",
       " 'text-embedding-ada-0028d54f3a4-5dde-56a9-b8a4-7569c596816e',\n",
       " 'text-embedding-ada-00208357f77-9067-5ce5-bf9c-23efc8c25891',\n",
       " 'text-embedding-ada-00259b28ef9-7b4a-500b-9215-9501647bd8fc',\n",
       " 'text-embedding-ada-00228867a97-824a-5c1a-bd18-af7bc4847df5',\n",
       " 'text-embedding-ada-002dd3c711a-87fb-54b0-ad65-1fb5e52594df',\n",
       " 'text-embedding-ada-002395c67cf-3f50-503c-9bf3-9133c316b8c8',\n",
       " 'text-embedding-ada-0022237d189-2154-5e1a-a5bc-7efa899bc231',\n",
       " 'text-embedding-ada-002e8f2d2ff-778f-5eff-a1ce-190c2211cd29',\n",
       " 'text-embedding-ada-002e49768b6-864e-598e-a057-1514e3a0197c',\n",
       " 'text-embedding-ada-002d285eae2-97b7-50e1-a9a8-dbb65e987625',\n",
       " 'text-embedding-ada-0029ba46e68-4024-51a0-8020-0d5cfe84705a',\n",
       " 'text-embedding-ada-00285cdf866-b432-5037-8d7f-76aa70e3f91d',\n",
       " 'text-embedding-ada-00288308a5d-b05b-53fb-9468-5b7037d4e8a1',\n",
       " 'text-embedding-ada-0029fd85f7c-c43c-531d-b392-29761e92e12c',\n",
       " 'text-embedding-ada-00213c613b9-9b19-5ee8-96e0-d3b822ce02e6',\n",
       " 'text-embedding-ada-002c5252378-886f-5d2b-a8a6-fcd254343607',\n",
       " 'text-embedding-ada-002d93a7073-e6ef-5574-8730-ac8ccd987267',\n",
       " 'text-embedding-ada-002a910d6a8-a54a-5df7-a0f9-91617768a701',\n",
       " 'text-embedding-ada-002863a9750-e453-58a5-b793-f3fc96c07243',\n",
       " 'text-embedding-ada-002424b6624-a211-5c90-a252-9062fb490650',\n",
       " 'text-embedding-ada-0029f11c1f0-f5e9-523a-ad10-eac102f57599',\n",
       " 'text-embedding-ada-002398d1cd5-6cf4-505e-b906-ce651377decb',\n",
       " 'text-embedding-ada-002091b1b2d-1ce6-5ec4-a028-f933dc809fb6',\n",
       " 'text-embedding-ada-0028a13aef7-3043-5f56-94ae-2e978059e3e1',\n",
       " 'text-embedding-ada-002fa2bd87a-fc5d-50a9-9676-0c2cc130797b',\n",
       " 'text-embedding-ada-002c16d635d-82d0-53ce-b72f-20d14401dce1',\n",
       " 'text-embedding-ada-002f5c2e554-ab84-555a-b539-572abc58f29f',\n",
       " 'text-embedding-ada-002c7c56ab5-4385-5682-812b-efee8e6e8e64',\n",
       " 'text-embedding-ada-00251b58770-7db0-5bd5-a9ae-eb9d0d57252c',\n",
       " 'text-embedding-ada-0021370c9c6-d53a-5277-8a19-03d20e6a3982',\n",
       " 'text-embedding-ada-002e3d80595-3ada-556d-80d2-9c60658dee8e',\n",
       " 'text-embedding-ada-002959b7570-279e-5108-8cd3-af984fc102bf',\n",
       " 'text-embedding-ada-002cef066d0-2552-5225-ae17-f0c1351f47cd',\n",
       " 'text-embedding-ada-0023f6c3a04-9d9f-5a40-8c70-e8b90ec71fc5',\n",
       " 'text-embedding-ada-00261a58d16-8ec1-5d4b-a752-5e836bf970b1',\n",
       " 'text-embedding-ada-002f2ed9db5-c832-58af-9ed8-98c560b20d7a',\n",
       " 'text-embedding-ada-002192dcd45-a894-51f7-9beb-7de41347ae37',\n",
       " 'text-embedding-ada-002c50425cb-a132-5e32-9f5d-e4292cbd289b',\n",
       " 'text-embedding-ada-00271e364e9-3fb7-5eec-811d-94838282e8e5',\n",
       " 'text-embedding-ada-0024bcca810-8a13-5ff9-96fe-ce097f6d231d',\n",
       " 'text-embedding-ada-002e9a45048-51bb-58f0-888c-aae09aeef689',\n",
       " 'text-embedding-ada-002a36075a8-3e06-5676-ae83-0b00ac3230f8',\n",
       " 'text-embedding-ada-002af89a638-18ed-57eb-bcfe-024da3a6a93c',\n",
       " 'text-embedding-ada-002edebd6ea-dbe2-5c9b-a399-4388d4749118',\n",
       " 'text-embedding-ada-002af54a85c-e04c-5aee-aedd-fb803a65fe7b',\n",
       " 'text-embedding-ada-00287fea93d-471a-532d-be76-678467f40157',\n",
       " 'text-embedding-ada-00227f5b227-c6ba-5d3b-9d0d-6b66b89449dd',\n",
       " 'text-embedding-ada-0027ce52a3c-07b0-5814-970b-efacece52aa4',\n",
       " 'text-embedding-ada-002be8f2e9e-46d4-57c7-8211-238616b27a05',\n",
       " 'text-embedding-ada-0021d97900e-98ba-53f0-b7b3-46d63203cfde',\n",
       " 'text-embedding-ada-002e4bb75a7-e6c3-514a-bfcf-633e01b8bb7c',\n",
       " 'text-embedding-ada-002d8e4dd30-33df-5b55-b2af-940b54fb5046',\n",
       " 'text-embedding-ada-002ba8e97c2-dea4-5eea-9294-d7ea0800d8a5',\n",
       " 'text-embedding-ada-002e17f11cd-e63a-50d3-bc6d-cf29343bedac',\n",
       " 'text-embedding-ada-0029377ef74-66df-5211-aafc-846378750ffb',\n",
       " 'text-embedding-ada-002c4f35bd1-31f1-523a-b05e-97acd8603b12',\n",
       " 'text-embedding-ada-0023c5ea95b-91eb-5a38-85e0-147821b9e606',\n",
       " 'text-embedding-ada-002b02c821e-2c53-58ac-96d6-66897b04ad2a',\n",
       " 'text-embedding-ada-002cec0c0f6-418b-5168-91d4-0b43b48f214b',\n",
       " 'text-embedding-ada-00289bdfd66-df6c-52bd-a962-62abb856985d',\n",
       " 'text-embedding-ada-002cdfdaaa6-49ec-588f-9e08-1ec66700abfc',\n",
       " 'text-embedding-ada-002884ca114-07ec-5ad6-8df9-48e08122b3b3',\n",
       " 'text-embedding-ada-0027615bcb2-46bc-5bd9-b50d-c8fb417f8821',\n",
       " 'text-embedding-ada-0024058fd8b-6bab-5978-8852-3da83e1a0643',\n",
       " 'text-embedding-ada-002da4271ac-537f-561e-a71f-5e624e67aedf',\n",
       " 'text-embedding-ada-0020f6ed6a1-ac53-5aa9-bd3c-f10bc7cc1a7c']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(store.yield_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_texts(korea, cached_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Can't use statement directly after '%%time'!\n"
     ]
    }
   ],
   "source": [
    "%%time #ì‹œê°„ ì¸¡ì •\n",
    "db2 = FAISS.from_texts(korea, cached_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text-embedding-ada-002a0146fd6-d992-5270-a7d0-4e0789f91cd1',\n",
       " 'text-embedding-ada-002218bc9da-cee2-586a-bc37-d41780259536',\n",
       " 'text-embedding-ada-002be8d00b4-6909-593c-a802-63ccb0a79efd',\n",
       " 'text-embedding-ada-00254317ce7-ea08-5df2-acc4-7ba7f72aeace',\n",
       " 'text-embedding-ada-00219bae6aa-dc2c-5fa3-9ca5-6905aabf4a78']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(store.yield_keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI(api_key=api_key,temperature=0.1)\n",
    "retriever = db.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['FAISS', 'CacheBackedEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x1249ec790>),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x30a56e850>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x30a5c25e0>, temperature=0.1, openai_api_key='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', openai_proxy='')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ë„¤, ì†í¥ë¯¼ì´ ìˆìŠµë‹ˆë‹¤.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"ì†í¥ë¯¼ì´ ìˆë‚˜ìš”?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('ì¼€ë¹ˆ ë”ë¸Œë¼ìœ„ë„ˆê°€ ëŒ€í‘œíŒ€ ëª…ë‹¨ì— ìˆë‚˜ìš”?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"í™©ì„ í™ì„ ìˆ˜ëŠ” ëª…ë‹¨ì— ìˆë‚˜ìš”?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ë‘'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"ê³µê²©ìˆ˜ë¡œ ì°¨ì¶œëœ ì„ ìˆ˜ëŠ” ëˆ„ê°€ ìˆë‚˜ìš”?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputs = RunnableParallel(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(api_key=api_key,temperature=0)\n",
    "    | StrOutputParser(),\n",
    ")\n",
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI(api_key=api_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the context provided, it is impossible to determine where Harrison worked.', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"where did harrison work?\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='It is not possible to determine where Harrison worked based on the given context.', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"where did he work?\",\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"Who wrote this notebook?\"),\n",
    "            AIMessage(content=\"Harrison\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”— Multiple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La ciudad de Chicago, Illinois, se encuentra en los Estados Unidos.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(api_key=api_key)\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"})\n",
    "\n",
    "\n",
    "\n",
    "#person ì´ ë¨¼ì € í˜¸ì¶œì´ë˜ì–´ì„œ chain1 ì„ ì‹¤í–‰ í•œ í›„ chain2ë¥¼ ì‹¤í–‰í•¨ ì§€ê¸ˆ ë¡œì§ìƒì€ city ë¥¼ ì¸ìê°’ì´ ë“¤ì–´ê°€ ìˆì§€ëŠ” ì•Šë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"generate a {attribute} color. Return the name of the color and nothing else:\"\n",
    ")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what is a fruit of color: {color}. Return the name of the fruit and nothing else:\"\n",
    ")\n",
    "prompt3 = ChatPromptTemplate.from_template(\n",
    "    \"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\"\n",
    ")\n",
    "prompt4 = ChatPromptTemplate.from_template(\n",
    "    \"What is the color of {fruit} and the flag of {country}?\"\n",
    ")\n",
    "\n",
    "model_parser = model | StrOutputParser()\n",
    "\n",
    "color_generator = (\n",
    "    {\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser}\n",
    ")\n",
    "color_to_fruit = prompt2 | model_parser\n",
    "color_to_country = prompt3 | model_parser\n",
    "question_generator = (\n",
    "    color_generator | {\"fruit\": color_to_fruit, \"country\": color_to_country} | prompt4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='What is the color of Orange and the flag of Bhutan?')])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_generator.invoke(\"warm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Both blueberry and the flag of Finland are blue in color.', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = question_generator.invoke('cooltone')\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying a SQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase   # eb ì—°ê²°\n",
    "db = SQLDatabase.from_uri(\"sqlite:///./Chinook.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(_):  # í…Œì´ë¸”ì˜ ì •ë³´ë¥¼ ê°€ì ¸ì˜´\n",
    "    return db.get_table_info()\n",
    "def run_query(query): # ì¿¼ë¦¬ë¥¼ ì‹¤í–‰\n",
    "    return db.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(api_key=api_key) \n",
    "\n",
    "sql_response = (\n",
    "    RunnablePassthrough.assign(schema=get_schema)\n",
    "    | prompt\n",
    "    | model.bind(stop=[\"\\nSQLResult:\"])\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT COUNT(employee_id) as total_employees \\nFROM employees;'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_response.invoke({\"question\": \"How many employees are there?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query} \n",
    "SQL Response: {response}\"\"\"\n",
    "prompt_response = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = (\n",
    "    RunnablePassthrough.assign(query=sql_response).assign(\n",
    "        schema=get_schema,\n",
    "        response=lambda x: db.run(x[\"query\"]),\n",
    "    )\n",
    "    | prompt_response\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) no such table: employees\n[SQL: SELECT COUNT(employee_id) \nFROM employees;]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1969\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1969\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdialect\u001b[39m.\u001b[39;49mdo_execute(\n\u001b[1;32m   1970\u001b[0m             cursor, str_statement, effective_parameters, context\n\u001b[1;32m   1971\u001b[0m         )\n\u001b[1;32m   1973\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_events \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39m_has_events:\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/sqlalchemy/engine/default.py:922\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_execute\u001b[39m(\u001b[39mself\u001b[39m, cursor, statement, parameters, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 922\u001b[0m     cursor\u001b[39m.\u001b[39;49mexecute(statement, parameters)\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: employees",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/Users/kdb/Desktop/RAG_examples/Cookbooks.ipynb ì…€ 49\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/Cookbooks.ipynb#Y105sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m full_chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mHow many employees are there?\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py:2218\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2216\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2217\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 2218\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   2219\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   2220\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2221\u001b[0m             patch_config(\n\u001b[1;32m   2222\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   2223\u001b[0m             ),\n\u001b[1;32m   2224\u001b[0m         )\n\u001b[1;32m   2225\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   2226\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/passthrough.py:453\u001b[0m, in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    436\u001b[0m         \u001b[39minput\u001b[39m, \u001b[39mdict\u001b[39m\n\u001b[1;32m    437\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    440\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m,\n\u001b[1;32m    441\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapper\u001b[39m.\u001b[39minvoke(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m         ),\n\u001b[1;32m    446\u001b[0m     }\n\u001b[1;32m    448\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m    449\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    450\u001b[0m     \u001b[39minput\u001b[39m: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m    451\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m--> 453\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    454\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_invoke, \u001b[39minput\u001b[39m, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    456\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_ainvoke\u001b[39m(\n\u001b[1;32m    457\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    458\u001b[0m     \u001b[39minput\u001b[39m: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    462\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py:1405\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1401\u001b[0m     context \u001b[39m=\u001b[39m copy_context()\n\u001b[1;32m   1402\u001b[0m     context\u001b[39m.\u001b[39mrun(var_child_runnable_config\u001b[39m.\u001b[39mset, child_config)\n\u001b[1;32m   1403\u001b[0m     output \u001b[39m=\u001b[39m cast(\n\u001b[1;32m   1404\u001b[0m         Output,\n\u001b[0;32m-> 1405\u001b[0m         context\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m   1406\u001b[0m             call_func_with_variable_args,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1407\u001b[0m             func,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1408\u001b[0m             \u001b[39minput\u001b[39;49m,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1409\u001b[0m             config,\n\u001b[1;32m   1410\u001b[0m             run_manager,\n\u001b[1;32m   1411\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1412\u001b[0m         ),\n\u001b[1;32m   1413\u001b[0m     )\n\u001b[1;32m   1414\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1415\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/passthrough.py:440\u001b[0m, in \u001b[0;36m_invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_invoke\u001b[39m(\n\u001b[1;32m    429\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    430\u001b[0m     \u001b[39minput\u001b[39m: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    434\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    435\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    436\u001b[0m         \u001b[39minput\u001b[39m, \u001b[39mdict\u001b[39m\n\u001b[1;32m    437\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m--> 440\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m,\n\u001b[1;32m    441\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapper\u001b[39m.\u001b[39minvoke(\n\u001b[1;32m    442\u001b[0m             \u001b[39minput\u001b[39m,\n\u001b[1;32m    443\u001b[0m             patch_config(config, callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child()),\n\u001b[1;32m    444\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    445\u001b[0m         ),\n\u001b[1;32m    446\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py:2855\u001b[0m, in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2846\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_input_schema\u001b[39m(\n\u001b[1;32m   2847\u001b[0m     \u001b[39mself\u001b[39m, config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2848\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Type[BaseModel]:\n\u001b[1;32m   2849\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m   2850\u001b[0m         s\u001b[39m.\u001b[39mget_input_schema(config)\u001b[39m.\u001b[39mschema()\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2851\u001b[0m         \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m   2852\u001b[0m     ):\n\u001b[1;32m   2853\u001b[0m         \u001b[39m# This is correct, but pydantic typings/mypy don't think so.\u001b[39;00m\n\u001b[1;32m   2854\u001b[0m         \u001b[39mreturn\u001b[39;00m create_model(  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m-> 2855\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_name(\u001b[39m\"\u001b[39m\u001b[39mInput\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   2856\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\n\u001b[1;32m   2857\u001b[0m                 k: (v\u001b[39m.\u001b[39mannotation, v\u001b[39m.\u001b[39mdefault)\n\u001b[1;32m   2858\u001b[0m                 \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m   2859\u001b[0m                 \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m step\u001b[39m.\u001b[39mget_input_schema(config)\u001b[39m.\u001b[39m__fields__\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   2860\u001b[0m                 \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__root__\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2861\u001b[0m             },\n\u001b[1;32m   2862\u001b[0m         )\n\u001b[1;32m   2864\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mget_input_schema(config)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py:2855\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2846\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_input_schema\u001b[39m(\n\u001b[1;32m   2847\u001b[0m     \u001b[39mself\u001b[39m, config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2848\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Type[BaseModel]:\n\u001b[1;32m   2849\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m   2850\u001b[0m         s\u001b[39m.\u001b[39mget_input_schema(config)\u001b[39m.\u001b[39mschema()\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2851\u001b[0m         \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m   2852\u001b[0m     ):\n\u001b[1;32m   2853\u001b[0m         \u001b[39m# This is correct, but pydantic typings/mypy don't think so.\u001b[39;00m\n\u001b[1;32m   2854\u001b[0m         \u001b[39mreturn\u001b[39;00m create_model(  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m-> 2855\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_name(\u001b[39m\"\u001b[39m\u001b[39mInput\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   2856\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\n\u001b[1;32m   2857\u001b[0m                 k: (v\u001b[39m.\u001b[39mannotation, v\u001b[39m.\u001b[39mdefault)\n\u001b[1;32m   2858\u001b[0m                 \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m   2859\u001b[0m                 \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m step\u001b[39m.\u001b[39mget_input_schema(config)\u001b[39m.\u001b[39m__fields__\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   2860\u001b[0m                 \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__root__\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2861\u001b[0m             },\n\u001b[1;32m   2862\u001b[0m         )\n\u001b[1;32m   2864\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mget_input_schema(config)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/concurrent/futures/_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    445\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py:3669\u001b[0m, in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3655\u001b[0m         output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39minvoke(\n\u001b[1;32m   3656\u001b[0m             \u001b[39minput\u001b[39m,\n\u001b[1;32m   3657\u001b[0m             patch_config(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3661\u001b[0m             ),\n\u001b[1;32m   3662\u001b[0m         )\n\u001b[1;32m   3663\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Output, output)\n\u001b[1;32m   3665\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_ainvoke\u001b[39m(\n\u001b[1;32m   3666\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   3667\u001b[0m     \u001b[39minput\u001b[39m: Input,\n\u001b[1;32m   3668\u001b[0m     run_manager: AsyncCallbackManagerForChainRun,\n\u001b[0;32m-> 3669\u001b[0m     config: RunnableConfig,\n\u001b[1;32m   3670\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m   3671\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Output:\n\u001b[1;32m   3672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mafunc\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   3673\u001b[0m         afunc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafunc\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py:1405\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1401\u001b[0m     context \u001b[39m=\u001b[39m copy_context()\n\u001b[1;32m   1402\u001b[0m     context\u001b[39m.\u001b[39mrun(var_child_runnable_config\u001b[39m.\u001b[39mset, child_config)\n\u001b[1;32m   1403\u001b[0m     output \u001b[39m=\u001b[39m cast(\n\u001b[1;32m   1404\u001b[0m         Output,\n\u001b[0;32m-> 1405\u001b[0m         context\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m   1406\u001b[0m             call_func_with_variable_args,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1407\u001b[0m             func,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1408\u001b[0m             \u001b[39minput\u001b[39;49m,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1409\u001b[0m             config,\n\u001b[1;32m   1410\u001b[0m             run_manager,\n\u001b[1;32m   1411\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1412\u001b[0m         ),\n\u001b[1;32m   1413\u001b[0m     )\n\u001b[1;32m   1414\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1415\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py:3543\u001b[0m, in \u001b[0;36m_invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3540\u001b[0m try:\n\u001b[1;32m   3541\u001b[0m     sig = inspect.signature(func)\n\u001b[1;32m   3542\u001b[0m     if sig.return_annotation != inspect.Signature.empty:\n\u001b[0;32m-> 3543\u001b[0m         # unwrap iterator types\n\u001b[1;32m   3544\u001b[0m         if getattr(sig.return_annotation, \"__origin__\", None) in (\n\u001b[1;32m   3545\u001b[0m             collections.abc.Iterator,\n\u001b[1;32m   3546\u001b[0m             collections.abc.AsyncIterator,\n\u001b[1;32m   3547\u001b[0m         ):\n\u001b[1;32m   3548\u001b[0m             return getattr(sig.return_annotation, \"__args__\", (Any,))[0]\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/Users/kdb/Desktop/RAG_examples/Cookbooks.ipynb ì…€ 49\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/Cookbooks.ipynb#Y105sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m full_chain \u001b[39m=\u001b[39m (\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/Cookbooks.ipynb#Y105sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     RunnablePassthrough\u001b[39m.\u001b[39massign(query\u001b[39m=\u001b[39msql_response)\u001b[39m.\u001b[39massign(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/Cookbooks.ipynb#Y105sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         schema\u001b[39m=\u001b[39mget_schema,\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/Cookbooks.ipynb#Y105sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         response\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: db\u001b[39m.\u001b[39;49mrun(x[\u001b[39m\"\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/Cookbooks.ipynb#Y105sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/Cookbooks.ipynb#Y105sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m|\u001b[39m prompt_response\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/Cookbooks.ipynb#Y105sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m|\u001b[39m model\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/Cookbooks.ipynb#Y105sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_community/utilities/sql_database.py:498\u001b[0m, in \u001b[0;36mSQLDatabase.run\u001b[0;34m(self, command, fetch, include_columns, parameters, execution_options)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\n\u001b[1;32m    485\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    486\u001b[0m     command: Union[\u001b[39mstr\u001b[39m, Executable],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m     execution_options: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[\u001b[39mstr\u001b[39m, Sequence[Dict[\u001b[39mstr\u001b[39m, Any]], Result[Any]]:\n\u001b[1;32m    493\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Execute a SQL command and return a string representing the results.\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \n\u001b[1;32m    495\u001b[0m \u001b[39m    If the statement returns rows, a string of the results is returned.\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[39m    If the statement returns no rows, an empty string is returned.\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute(\n\u001b[1;32m    499\u001b[0m         command, fetch, parameters\u001b[39m=\u001b[39;49mparameters, execution_options\u001b[39m=\u001b[39;49mexecution_options\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m     \u001b[39mif\u001b[39;00m fetch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcursor\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    503\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_community/utilities/sql_database.py:463\u001b[0m, in \u001b[0;36mSQLDatabase._execute\u001b[0;34m(self, command, fetch, parameters, execution_options)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mQuery expression has unknown type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(command)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 463\u001b[0m cursor \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    464\u001b[0m     command,\n\u001b[1;32m    465\u001b[0m     parameters,\n\u001b[1;32m    466\u001b[0m     execution_options\u001b[39m=\u001b[39;49mexecution_options,\n\u001b[1;32m    467\u001b[0m )\n\u001b[1;32m    469\u001b[0m \u001b[39mif\u001b[39;00m cursor\u001b[39m.\u001b[39mreturns_rows:\n\u001b[1;32m    470\u001b[0m     \u001b[39mif\u001b[39;00m fetch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1416\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1414\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mObjectNotExecutableError(statement) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1416\u001b[0m     \u001b[39mreturn\u001b[39;00m meth(\n\u001b[1;32m   1417\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1418\u001b[0m         distilled_parameters,\n\u001b[1;32m   1419\u001b[0m         execution_options \u001b[39mor\u001b[39;49;00m NO_OPTIONS,\n\u001b[1;32m   1420\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/sqlalchemy/sql/elements.py:516\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[0;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m    515\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, Executable)\n\u001b[0;32m--> 516\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\u001b[39m.\u001b[39;49m_execute_clauseelement(\n\u001b[1;32m    517\u001b[0m         \u001b[39mself\u001b[39;49m, distilled_params, execution_options\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mObjectNotExecutableError(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1639\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[0;34m(self, elem, distilled_parameters, execution_options)\u001b[0m\n\u001b[1;32m   1627\u001b[0m compiled_cache: Optional[CompiledCacheType] \u001b[39m=\u001b[39m execution_options\u001b[39m.\u001b[39mget(\n\u001b[1;32m   1628\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcompiled_cache\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39m_compiled_cache\n\u001b[1;32m   1629\u001b[0m )\n\u001b[1;32m   1631\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_compile_w_cache(\n\u001b[1;32m   1632\u001b[0m     dialect\u001b[39m=\u001b[39mdialect,\n\u001b[1;32m   1633\u001b[0m     compiled_cache\u001b[39m=\u001b[39mcompiled_cache,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1637\u001b[0m     linting\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdialect\u001b[39m.\u001b[39mcompiler_linting \u001b[39m|\u001b[39m compiler\u001b[39m.\u001b[39mWARN_LINTING,\n\u001b[1;32m   1638\u001b[0m )\n\u001b[0;32m-> 1639\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_context(\n\u001b[1;32m   1640\u001b[0m     dialect,\n\u001b[1;32m   1641\u001b[0m     dialect\u001b[39m.\u001b[39;49mexecution_ctx_cls\u001b[39m.\u001b[39;49m_init_compiled,\n\u001b[1;32m   1642\u001b[0m     compiled_sql,\n\u001b[1;32m   1643\u001b[0m     distilled_parameters,\n\u001b[1;32m   1644\u001b[0m     execution_options,\n\u001b[1;32m   1645\u001b[0m     compiled_sql,\n\u001b[1;32m   1646\u001b[0m     distilled_parameters,\n\u001b[1;32m   1647\u001b[0m     elem,\n\u001b[1;32m   1648\u001b[0m     extracted_params,\n\u001b[1;32m   1649\u001b[0m     cache_hit\u001b[39m=\u001b[39;49mcache_hit,\n\u001b[1;32m   1650\u001b[0m )\n\u001b[1;32m   1651\u001b[0m \u001b[39mif\u001b[39;00m has_events:\n\u001b[1;32m   1652\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mafter_execute(\n\u001b[1;32m   1653\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m   1654\u001b[0m         elem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1658\u001b[0m         ret,\n\u001b[1;32m   1659\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1848\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1843\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exec_insertmany_context(\n\u001b[1;32m   1844\u001b[0m         dialect,\n\u001b[1;32m   1845\u001b[0m         context,\n\u001b[1;32m   1846\u001b[0m     )\n\u001b[1;32m   1847\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1848\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_exec_single_context(\n\u001b[1;32m   1849\u001b[0m         dialect, context, statement, parameters\n\u001b[1;32m   1850\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1988\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1985\u001b[0m     result \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39m_setup_result_proxy()\n\u001b[1;32m   1987\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1988\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_dbapi_exception(\n\u001b[1;32m   1989\u001b[0m         e, str_statement, effective_parameters, cursor, context\n\u001b[1;32m   1990\u001b[0m     )\n\u001b[1;32m   1992\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/sqlalchemy/engine/base.py:2343\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[1;32m   2341\u001b[0m \u001b[39melif\u001b[39;00m should_wrap:\n\u001b[1;32m   2342\u001b[0m     \u001b[39massert\u001b[39;00m sqlalchemy_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2343\u001b[0m     \u001b[39mraise\u001b[39;00m sqlalchemy_exception\u001b[39m.\u001b[39mwith_traceback(exc_info[\u001b[39m2\u001b[39m]) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   2344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2345\u001b[0m     \u001b[39massert\u001b[39;00m exc_info[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1969\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1967\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1968\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1969\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdialect\u001b[39m.\u001b[39;49mdo_execute(\n\u001b[1;32m   1970\u001b[0m             cursor, str_statement, effective_parameters, context\n\u001b[1;32m   1971\u001b[0m         )\n\u001b[1;32m   1973\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_events \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39m_has_events:\n\u001b[1;32m   1974\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mafter_cursor_execute(\n\u001b[1;32m   1975\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m   1976\u001b[0m         cursor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1980\u001b[0m         context\u001b[39m.\u001b[39mexecutemany,\n\u001b[1;32m   1981\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/sqlalchemy/engine/default.py:922\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_execute\u001b[39m(\u001b[39mself\u001b[39m, cursor, statement, parameters, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 922\u001b[0m     cursor\u001b[39m.\u001b[39;49mexecute(statement, parameters)\n",
      "\u001b[0;31mOperationalError\u001b[0m: (sqlite3.OperationalError) no such table: employees\n[SQL: SELECT COUNT(employee_id) \nFROM employees;]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"How many employees are there?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, tool\n",
    "from langchain.agents.output_parsers import XMLAgentOutputParser\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "claude_api_key = 'your_api_key'\n",
    "model = ChatAnthropic(model=\"claude-2\",anthropic_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search things about current events.\"\"\"\n",
    "    return \"32 degrees\"\n",
    "\n",
    "tool_list = [search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"hwchase17/xml-agent-convo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['agent_scratchpad', 'input', 'tools'] partial_variables={'chat_history': ''} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'chat_history', 'input', 'tools'], template=\"You are a helpful assistant. Help the user answer any questions.\\n\\nYou have access to the following tools:\\n\\n{tools}\\n\\nIn order to use a tool, you can use <tool></tool> and <tool_input></tool_input> tags. You will then get back a response in the form <observation></observation>\\nFor example, if you have a tool called 'search' that could run a google search, in order to search for the weather in SF you would respond:\\n\\n<tool>search</tool><tool_input>weather in SF</tool_input>\\n<observation>64 degrees</observation>\\n\\nWhen you are done, respond with a final answer between <final_answer></final_answer>. For example:\\n\\n<final_answer>The weather in SF is 64 degrees</final_answer>\\n\\nBegin!\\n\\nPrevious Conversation:\\n{chat_history}\\n\\nQuestion: {input}\\n{agent_scratchpad}\"))]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic for going from intermediate steps to a string to pass into model\n",
    "# This is pretty tied to the prompt\n",
    "def convert_intermediate_steps(intermediate_steps):\n",
    "    log = \"\"\n",
    "    for action, observation in intermediate_steps:\n",
    "        log += (\n",
    "            f\"<tool>{action.tool}</tool><tool_input>{action.tool_input}\"\n",
    "            f\"</tool_input><observation>{observation}</observation>\"\n",
    "        )\n",
    "    return log\n",
    "\n",
    "\n",
    "# Logic for converting tools to string to go in prompt\n",
    "def convert_tools(tools):\n",
    "    return \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: convert_intermediate_steps(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt.partial(tools=convert_tools(tool_list))\n",
    "    | model.bind(stop=[\"</tool_input>\", \"</final_answer>\"])\n",
    "    | XMLAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tool_list, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m <tool>search</tool><tool_input>ì˜¤íƒ€ë‹ˆì˜ ìµœê·¼ì„±ì \u001b[0m\u001b[36;1m\u001b[1;3m32 degrees\u001b[0m\u001b[32;1m\u001b[1;3m <tool>search</tool><tool_input>ì˜¤íƒ€ë‹ˆì˜ ìµœê·¼ì„±ì \u001b[0m\u001b[36;1m\u001b[1;3m32 degrees\u001b[0m\u001b[32;1m\u001b[1;3m <tool>search</tool><tool_input>ì˜¤íƒ€ë‹ˆì˜ ìµœê·¼ì„±ì \u001b[0m\u001b[36;1m\u001b[1;3m32 degrees\u001b[0m\u001b[32;1m\u001b[1;3m <tool>search</tool><tool_input>ì˜¤íƒ€ë‹ˆì˜ ìµœê·¼ì„±ì \u001b[0m\u001b[36;1m\u001b[1;3m32 degrees\u001b[0m\u001b[32;1m\u001b[1;3m <tool>search</tool><tool_input>ì˜¤íƒ€ë‹ˆì˜ ìµœê·¼ ì„±ì \u001b[0m\u001b[36;1m\u001b[1;3m32 degrees\u001b[0mUnexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/d4/56xjd1v51xq5ybs0jdmq217h0000gn/T/ipykernel_45393/3644363406.py\", line 1, in <module>\n",
      "    agent_executor.invoke({\"input\": \"ì˜¤íƒ€ë‹ˆì˜ ìµœê·¼ì„±ì ì€ ì–´ë•Œ?\"})\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain/chains/base.py\", line 163, in invoke\n",
      "    raise e\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain/chains/base.py\", line 153, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain/agents/agent.py\", line 1432, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain/agents/agent.py\", line 1138, in _take_next_step\n",
      "    [\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain/agents/agent.py\", line 1138, in <listcomp>\n",
      "    [\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain/agents/agent.py\", line 1166, in _iter_next_step\n",
      "    output = self.agent.plan(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain/agents/agent.py\", line 397, in plan\n",
      "    for chunk in self.runnable.stream(inputs, config={\"callbacks\": callbacks}):\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 2589, in stream\n",
      "    )\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 2576, in transform\n",
      "    else:\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 1656, in _transform_stream_with_config\n",
      "    for callback_manager, input, config in zip(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 2540, in _transform\n",
      "    remaining_idxs = [\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 1180, in transform\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 4430, in transform\n",
      "    inputs,\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 1197, in transform\n",
      "    input: Iterator[Input],\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\", line 258, in stream\n",
      "    await self.ainvoke(input, config=config, stop=stop, **kwargs),\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\", line 241, in stream\n",
      "    run_manager.on_llm_end(LLMResult(generations=[[generation]]))\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_community/chat_models/anthropic.py\", line 148, in _stream\n",
      "    stream_resp = self.client.completions.create(**params, stream=True)\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/resources/completions.py\", line 385, in create\n",
      "    return self._post(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_base_client.py\", line 1208, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_base_client.py\", line 897, in request\n",
      "    return self._request(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_base_client.py\", line 973, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_base_client.py\", line 1021, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_base_client.py\", line 973, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_base_client.py\", line 1021, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_base_client.py\", line 988, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "anthropic.RateLimitError: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of requests has exceeded your rate limit (https://docs.anthropic.com/claude/reference/rate-limits). Please try again later or contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2142, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"ì˜¤íƒ€ë‹ˆì˜ ìµœê·¼ì„±ì ì€ ì–´ë•Œ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/d4/56xjd1v51xq5ybs0jdmq217h0000gn/T/ipykernel_45393/3694676024.py\", line 1, in <module>\n",
      "    agent_executor.invoke({\"input\": \"whats the weather in New york?\"})\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain/chains/base.py\", line 163, in invoke\n",
      "    raise e\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain/chains/base.py\", line 153, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain/agents/agent.py\", line 1432, in _call\n",
      "    next_step_output = self._take_next_step(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain/agents/agent.py\", line 1138, in _take_next_step\n",
      "    [\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain/agents/agent.py\", line 1138, in <listcomp>\n",
      "    [\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain/agents/agent.py\", line 1166, in _iter_next_step\n",
      "    output = self.agent.plan(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain/agents/agent.py\", line 397, in plan\n",
      "    for chunk in self.runnable.stream(inputs, config={\"callbacks\": callbacks}):\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 2589, in stream\n",
      "    )\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 2576, in transform\n",
      "    else:\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 1656, in _transform_stream_with_config\n",
      "    for callback_manager, input, config in zip(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 2540, in _transform\n",
      "    remaining_idxs = [\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 1180, in transform\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 4430, in transform\n",
      "    inputs,\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 1197, in transform\n",
      "    input: Iterator[Input],\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\", line 258, in stream\n",
      "    await self.ainvoke(input, config=config, stop=stop, **kwargs),\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\", line 241, in stream\n",
      "    run_manager.on_llm_end(LLMResult(generations=[[generation]]))\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_community/chat_models/anthropic.py\", line 148, in _stream\n",
      "    stream_resp = self.client.completions.create(**params, stream=True)\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_utils/_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/resources/completions.py\", line 385, in create\n",
      "    return self._post(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_base_client.py\", line 1208, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_base_client.py\", line 897, in request\n",
      "    return self._request(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_base_client.py\", line 973, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_base_client.py\", line 1021, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_base_client.py\", line 973, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_base_client.py\", line 1021, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/anthropic/_base_client.py\", line 988, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "anthropic.RateLimitError: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of requests has exceeded your rate limit (https://docs.anthropic.com/claude/reference/rate-limits). Please try again later or contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2142, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/kdb/anaconda3/envs/kdb/lib/python3.9/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"whats the weather in New york?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "#í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì´ë‹¤.\n",
    "template = \"\"\"Write some python code to solve the user's problem. \n",
    "\n",
    "Return only python code in Markdown format, e.g.:\n",
    "\n",
    "```python\n",
    "....\n",
    "```\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])\n",
    "\n",
    "model = ChatOpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sanitize_output(text: str):\n",
    "    _, after = text.split(\"```python\") # ```pythonì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ”\n",
    "    return after.split(\"```\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser() | _sanitize_output | PythonREPL().run #python ì½”ë“œë¥¼ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6\\n'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"whats 2 multiplied by 3?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing by semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_api_key = 'your_api_key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "#ë¬¼ë¦¬í•™ ì „ë¬¸ê°€ í…œí”Œë¦¿\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\ \n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "#ìˆ˜í•™ ì „ë¬¸ê°€ í…œí”Œë¦¿\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key = \"your_openai_api_key\")\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "\n",
    "def prompt_router(input):\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"]) #ì¿¼ë¦¬ë¥¼ ë°›ì•„ì„œ ì„ë² ë”©ì„ í•œë‹¤.\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0] # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì¿¼ë¦¬ì— ëŒ€í•œ ì„ë² ë”©ê³¼ í…œí”Œë¦¿ ì„ë² ë”©ì„ ë¹„êµí•œë‹¤.\n",
    "    most_similar = prompt_templates[similarity.argmax()] #ê°€ì¥ ìœ ì‚¬í•œ í…œí”Œë¦¿ì„ ì„ íƒí•œë‹¤.\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\") #ìˆ˜í•™ì´ë‚˜ ë¬¼ë¦¬í•™ í…œí”Œë¦¿ì„ ì‚¬ìš©í•œë‹¤.\n",
    "    return PromptTemplate.from_template(most_similar) #ê°€ì¥ ìœ ì‚¬í•œ í…œí”Œë¦¿ì„ ë°˜í™˜í•œë‹¤.\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | ChatOpenAI(api_key=op_api_key)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "The first person to land on the moon was Neil Armstrong on July 20, 1969 as part of the Apollo 11 mission.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"ìµœì´ˆë¡œ ë‹¬ì— ì°©ë¥™í•œì‚¬ëŒ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MATH\n",
      "A partial differential equation is an equation that involves partial derivatives of an unknown function of several independent variables. These types of equations are commonly used to model physical phenomena in areas such as physics, engineering, and economics. The solution to a partial differential equation is a function that satisfies the equation and its boundary or initial conditions.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"What is a partial differential?\")) #í¸ë¯¸ë¶„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(api_key = op_api_key)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True) #ëŒ€í™”í˜• ë©”ëª¨ë¦¬\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, ê¹€ë•ë°°ë‹˜! ì¼€ë¹ˆ ë” ë¸Œë¼ìš°ë„ˆë¥¼ ì¢‹ì•„í•˜ëŠ”êµ°ìš”. ê·¸ì˜ ì˜í™”ë‚˜ ìŒì•… ë“± ë¬´ì—‡ì„ ì¢‹ì•„í•˜ì‹œë‚˜ìš”? ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"input\": \"ë‚´ì´ë¦„ì€ ê¹€ë•ë°°ì•¼ ì•ˆë…•. ë‚˜ëŠ” ì¼€ë¹ˆë”ë¸Œë¼ìœ„ë„ˆë¥¼ ì¢‹ì•„í•´\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs, {\"output\": response.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi im bob'),\n",
       "  AIMessage(content='Hello Bob! How can I assist you today?'),\n",
       "  HumanMessage(content='ë‚´ì´ë¦„ì€ ê¹€ë•ë°°ì•¼ ì•ˆë…•. ë‚˜ëŠ” ì¼€ë¹ˆë”ë¸Œë¼ìœ„ë„ˆë¥¼ ì¢‹ì•„í•´'),\n",
       "  AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, ê¹€ë•ë°°ë‹˜! ì¼€ë¹ˆ ë” ë¸Œë¼ìš°ë„ˆë¥¼ ì¢‹ì•„í•˜ëŠ”êµ°ìš”. ê·¸ì˜ ì˜í™”ë‚˜ ìŒì•… ë“± ë¬´ì—‡ì„ ì¢‹ì•„í•˜ì‹œë‚˜ìš”? ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?')]}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì£„ì†¡í•©ë‹ˆë‹¤, í˜¼ë€ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤. ì•ì„œ ë§ì”€í•˜ì‹  ëŒ€ë¡œ ì¼€ë¹ˆ ë” ë¸Œë¼ìš°ë„ˆë¥¼ ì¢‹ì•„í•˜ì‹ ë‹¤ê³  í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”!', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"input\": \"ë‚´ê°€ ëˆ„êµ¬ë¥¼ ì¢‹ì•„í•œë‹¤ê³ ?\"}\n",
    "response = chain.invoke(inputs) #bobì´ë€ê±¸ ì•Œê³  ëŒ€ë‹µí•œë‹¤.\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIModerationChain\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/kdb/Desktop/RAG_examples/Cookbooks.ipynb ì…€ 77\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/Cookbooks.ipynb#Y146sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompts\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatPromptTemplate\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/Cookbooks.ipynb#Y146sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_openai\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAI\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/Cookbooks.ipynb#Y146sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m moderate \u001b[39m=\u001b[39m OpenAIModerationChain(api_key\u001b[39m=\u001b[39;49mop_api_key)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAIModerationChain\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.chains import OpenAIModerationChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "moderate = OpenAIModerationChain(api_key=op_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gpt-researcher 0.0.8 requires duckduckgo-search==3.9.8, but you have duckduckgo-search 5.1.0 which is incompatible.\n",
      "gpt-researcher 0.0.8 requires langchain==0.0.335, but you have langchain 0.1.13 which is incompatible.\n",
      "gpt-researcher 0.0.8 requires openai~=1.3.3, but you have openai 1.12.0 which is incompatible.\n",
      "gpt-researcher 0.0.8 requires tavily-python==0.2.8, but you have tavily-python 0.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-openai duckduckgo-search\n",
    "# duckduckgo-search ê²€ìƒ‰ì—”ì§„ì´ë‹¤.\n",
    "# í™œìš©í•  ìˆ˜ ìˆëŠ”ê²Œ ë§ìŒ\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchRun()\n",
    "template = \"\"\"turn the following user input into a search query for a search engine:\n",
    "\n",
    "{input}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(api_key=op_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser() | search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ê´€ë ¨ê¸€. ì˜¤ëŠ˜ì€ ì§€ë‚œ ì£¼ë§ì— ì—´ë ¸ë˜ ì‰ê¸€ëœë“œ í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸ 25ë¼ìš´ë“œ ê²°ê³¼ ë° íŒ€ ìˆœìœ„ì™€ ëŒì•„ì˜¤ëŠ” ì£¼ë§ì— ì¹˜ëŸ¬ì§€ëŠ” 26ë¼ìš´ë“œ ì¼ì •ì„ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸ 25ë¼ìš´ë“œ ê²°ê³¼ ë¸Œë ŒíŠ¸í¬ë“œ 1 - 4 ë¦¬ë²„í’€ ë²ˆë¦¬ 0 - 5 ì•„ìŠ¤ë‚  í’€ëŸ¼ 1 - 2 ì•„ìŠ¤í†¤ë¹Œë¼ ë‰´ìºìŠ¬ 2 - 2 ... ì •ì‹ ëª…ì¹­ì€ 'Premier League' ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ í¸ì˜ìƒ 'EPL' ì„ ì“´ë‹¤ê³  í•˜ë„¤ìš”. ì´ 20ê°œíŒ€ì´ ë¦¬ê·¸ë¥¼ ì¹˜ë£¨ê²Œ ë©ë‹ˆë‹¤. í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸(EPL)ëŠ” 1992ë…„ 2ì›”ì— ì°½ì„¤ë˜ì–´. ì˜¬í•´ 31ì£¼ë…„ì„ ë§ì´í•œ ë¦¬ê·¸ ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìµœë‹¤ ìš°ìŠ¹íŒ€ì€ ë§¨ìœ ë¡œ ì´ 20íšŒ ìš°ìŠ¹ ì„ í–ˆê³ , 1. 2021-2022 English Premier League(EPL) ì‹œì¦Œ ê°œìš” 2021-2022 English Premier League(EPL) ì‹œì¦Œì€ ë§¤ë…„ ì—´ë¦¬ëŠ” ì‰ê¸€ëœë“œ í”„ë¡œí˜ì…”ë„ ë¦¬ê·¸ 1ë¶€ ë¦¬ê·¸ì¸ EPLì˜ ìƒˆë¡œìš´ ì‹œì¦Œì„ ë§ì´í•˜ëŠ” ê²ƒì´ë‹¤. ì´ ì‹œì¦Œì—ëŠ” ì‰ê¸€ëœë“œì˜ ê° ì§€ì—­ì—ì„œ ì˜¨ 20ê°œì˜ í´ëŸ½ì´ ì°¸ê°€í•˜ì—¬ ì´ 380ê²½ê¸°ê°€ í¼ì³ì§ˆ ì˜ˆì •ì´ë‹¤. ì´ë²ˆì£¼ í† íŠ¸ë„˜ ê²½ê¸°ì¼ì • í† íŠ¸ë„˜ì€ 17ì¼ ì˜¤ì „ 2ì‹œ 30ë¶„ EPL 29ë¼ìš´ë“œ ì›ì •ê²½ê¸°ì—ì„œ í’€ëŸ¼FCì™€ ê²©ëŒí•˜ëŠ”ë°ìš”. ìµœê·¼ ê¸°ì„¸ê°€ ë§¤ì„œìš´ ìº¡í‹´ ì†í¥ë¯¼ì„ ìˆ˜ê°€ êµ­ê°€ëŒ€í‘œíŒ€ ì†Œì§‘ì„ ì•ë‘ê³  3ê²½ê¸° ì—°ì†ê³¨ì„ ì •ì¡°ì¤€í•©ë‹ˆë‹¤. ì•„ì‹œì•„ì¸ ìµœì´ˆ í•œ ê²½ê¸° 4 ... eplì€ í•œêµ­íŒ¬ë“¤ì—ê² ê°€ì¥ ì¸ê¸° ìˆëŠ” ìœ ëŸ½ ë¦¬ê·¸ ì¤‘ í•˜ë‚˜ì¸ë°, ... ì†í¥ë¯¼ì´ 2016-17 ì‹œì¦Œ 14ê³¨, 2020-21 ì‹œì¦Œ 17ê³¨, 2021-22 ì‹œì¦Œ 23ê³¨ì„ ê¸°ë¡í•˜ë©° ì¬ì°¨ ê²½ì‹ í–ˆë‹¤. ì´ëŠ” ì•„ì‹œì•„ì¸ ìµœì´ˆì˜ ë‘ ìë¦¬ìˆ˜ ë“ì ì´ë©° í•œêµ­ì¸ì„ì„ ë– ë‚˜ ì•„ì‹œì•„ì¸ìœ¼ë¡œì„œë„ í° ...\""
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"ì´ë²ˆì£¼ì— EPL ê²½ê¸°ì¼ì •ì„ ì•Œê³ ì‹¶ì–´!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

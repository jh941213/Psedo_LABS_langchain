{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RunnablePassthroughë¥¼ ì‚¬ìš©í•˜ë©´ ì…ë ¥ì„ ë³€ê²½í•˜ì§€ ì•Šê±°ë‚˜ ì¶”ê°€ í‚¤ë¥¼ ì¶”ê°€í•˜ì—¬ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë§µì˜ ìƒˆ í‚¤ì— ë°ì´í„°ë¥¼ í• ë‹¹í•˜ê¸° ìœ„í•´ RunnableParallelê³¼ í•¨ê»˜ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "- ë‹¨ë…ìœ¼ë¡œ í˜¸ì¶œë˜ëŠ” RunnablePassthrough()ëŠ” ë‹¨ìˆœíˆ ì…ë ¥ì„ ë°›ì•„ í†µê³¼ì‹œí‚µë‹ˆë‹¤.\n",
    "\n",
    "- í• ë‹¹ê³¼ í•¨ê»˜ í˜¸ì¶œëœ RunnablePassthrough(.assign(...))ëŠ” ì…ë ¥ì„ ë°›ì€ í›„ í• ë‹¹ í•¨ìˆ˜ì— ì „ë‹¬ëœ ì¶”ê°€ ì¸ìˆ˜ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['name'], template='{name} ì´ ì‚¬ëŒì€ ëˆ„êµ¬ì•¼?')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{name} ì´ ì‚¬ëŒì€ ëˆ„êµ¬ì•¼?\")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_api_key = \"youar_api_key_here\"\n",
    "\n",
    "model = ChatOpenAI(model = 'gpt-3.5-turbo-0125',api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='íŒ€ ì¿¡(Tim Cook)ì€ ì• í”Œ(Apple)ì˜ CEOì´ë©°, ìŠ¤í‹°ë¸Œ ì¡ìŠ¤(Steve Jobs)ì˜ í›„ê³„ìë¡œì„œ ì• í”Œì„ ì´ëŒê³  ìˆëŠ” ì¤‘ìš”í•œ ì¸ë¬¼ì…ë‹ˆë‹¤. ê·¸ëŠ” ì• í”Œì—ì„œ 1998ë…„ë¶€í„° ê·¼ë¬´í•˜ë©° ì£¼ìš”í•œ ì—­í• ì„ ë§¡ì•„ ì™”ê³ , 2011ë…„ë¶€í„° CEOë¡œ ì„ëª…ë˜ì–´ ì• í”Œì˜ ì„±ê³µê³¼ í˜ì‹ ì„ ì´ëŒì–´ ì™”ìŠµë‹ˆë‹¤. í˜„ì¬ ì• í”Œì€ ì„¸ê³„ì ìœ¼ë¡œ ê°€ì¥ í° ê¸°ìˆ  ê¸°ì—… ì¤‘ í•˜ë‚˜ë¡œ ìë¦¬ë§¤ê¹€í•˜ê³  ìˆìŠµë‹ˆë‹¤.', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"name\":\"tim cook\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected mapping type as input to PromptTemplate. Received <class 'str'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kdb/Desktop/RAG_examples/pseudo_lab_2weeks_method.ipynb ì…€ 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/pseudo_lab_2weeks_method.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m chain\u001b[39m.\u001b[39;49minvoke(\u001b[39m\"\u001b[39;49m\u001b[39mì˜¤ë°”ë§ˆ\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m# ì—ëŸ¬ê°€ë‚˜ëŠ”ë°, dict í˜•íƒœë¡œ key ê°’ì¸ \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py:2218\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2216\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2217\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 2218\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   2219\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   2220\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2221\u001b[0m             patch_config(\n\u001b[1;32m   2222\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   2223\u001b[0m             ),\n\u001b[1;32m   2224\u001b[0m         )\n\u001b[1;32m   2225\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   2226\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/prompts/base.py:113\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtags:\n\u001b[1;32m    112\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39mtags\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtags)\n\u001b[0;32m--> 113\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[1;32m    114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_format_prompt_with_error_handling,\n\u001b[1;32m    115\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    116\u001b[0m     config,\n\u001b[1;32m    117\u001b[0m     run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    118\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py:1405\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1401\u001b[0m     context \u001b[39m=\u001b[39m copy_context()\n\u001b[1;32m   1402\u001b[0m     context\u001b[39m.\u001b[39mrun(var_child_runnable_config\u001b[39m.\u001b[39mset, child_config)\n\u001b[1;32m   1403\u001b[0m     output \u001b[39m=\u001b[39m cast(\n\u001b[1;32m   1404\u001b[0m         Output,\n\u001b[0;32m-> 1405\u001b[0m         context\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m   1406\u001b[0m             call_func_with_variable_args,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1407\u001b[0m             func,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1408\u001b[0m             \u001b[39minput\u001b[39;49m,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1409\u001b[0m             config,\n\u001b[1;32m   1410\u001b[0m             run_manager,\n\u001b[1;32m   1411\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1412\u001b[0m         ),\n\u001b[1;32m   1413\u001b[0m     )\n\u001b[1;32m   1414\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1415\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/prompts/base.py:92\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_format_prompt_with_error_handling\u001b[39m(\u001b[39mself\u001b[39m, inner_input: Dict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[1;32m     91\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(inner_input, \u001b[39mdict\u001b[39m):\n\u001b[0;32m---> 92\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m     93\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected mapping type as input to \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(inner_input)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m         )\n\u001b[1;32m     96\u001b[0m     missing \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables)\u001b[39m.\u001b[39mdifference(inner_input)\n\u001b[1;32m     97\u001b[0m     \u001b[39mif\u001b[39;00m missing:\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected mapping type as input to PromptTemplate. Received <class 'str'>."
     ]
    }
   ],
   "source": [
    "chain.invoke(\"ì˜¤ë°”ë§ˆ\") # ì—ëŸ¬ê°€ë‚˜ëŠ”ë°, dict í˜•íƒœë¡œ key ê°’ì¸ name : \"ì˜¤ë°”ë§ˆ\" ì˜ í˜•íƒœë¡œ ì…ë ¥ì„ í•˜ì§€ ì•Šì•„ì„œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Kevin De BruyneëŠ” ë²¨ê¸°ì— ì¶œì‹ ì˜ ì¶•êµ¬ ì„ ìˆ˜ë¡œ ë§¨ì²´ìŠ¤í„° ì‹œí‹°ì—ì„œ ë›°ê³  ìˆëŠ” ì¤‘í•„ë”ì´ë‹¤. ê·¸ëŠ” ë›°ì–´ë‚œ íŒ¨ìŠ¤ì™€ ìŠˆíŒ… ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ë§¨ì²´ìŠ¤í„° ì‹œí‹°ì™€ ë²¨ê¸°ì— ëŒ€í‘œíŒ€ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê³  ìˆëŠ” ì„ ìˆ˜ì´ë‹¤. í˜„ì¬ ì„¸ê³„ì ìœ¼ë¡œë„ ìµœê³ ì˜ ì¤‘í•„ë” ì¤‘ í•œ ëª…ìœ¼ë¡œ ê¼½íˆê³  ìˆë‹¤.', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_2 = {\"name\": RunnablePassthrough()} | prompt | model #name ì´ë¼ëŠ” key ê°’ìœ¼ë¡œ RunnablePassthroughë¥¼ ë„£ì–´ì¤€ë‹¤. \n",
    "chain_2.invoke(\"kevin de bruyne\") #invoke ì—ì„œ ë”°ë¡œ ì„¤ì •ì—†ì´ ìŠ¤íŠ¸ë§ë§Œ ì…ë ¥í•˜ë©´ ê²°ê³¼ë¥¼ ì¶œë ¥ì‹œí‚¬ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAGì—ì„œì˜ ì˜ˆì œ\n",
    "Chain = {\"document\":retriver, \"question\": RunnablePassthrough()} | prompt | model\n",
    "prompt = \"{retriever}ì—ì„œ {question}ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì•„ì£¼ì„¸ìš”.\"\n",
    "#ì´ëŸ¬í•œ í˜•íƒœë¡œ RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•´ì„œ ë¦¬íŠ¸ë¦¬ë²„ì™€ ëª¨ë¸ì„ ì—°ê²°í•´ì¤„ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¦œexamples ì—ì„œì˜ ì˜ˆì œë¥¼ ë´ë³´ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "runnable = RunnableParallel( #ì½”ë“œë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í• ë•Œ ì“°ëŠ” ëŸ°ì–´ë¸” íŒ¨ëŸ´\n",
    "    passed=RunnablePassthrough(), # 1\n",
    "    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3), # 1, mult =3 #assign ì„ í†µí•´ ê°’ì„ ë„£ì–´ì¤„ ìˆ˜ ìˆë‹¤.\n",
    "    modified=lambda x: x[\"num\"] + 1,# 2\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnableParallel\n",
    "RunnableParallelì€ ì‹œí€€ìŠ¤ì—ì„œ ë‹¤ìŒ ëŸ°ì²˜ë¸”ì˜ ì…ë ¥ í˜•ì‹ê³¼ ì¼ì¹˜í•˜ë„ë¡ í•œ ëŸ°ì²˜ë¸”ì˜ ì¶œë ¥ì„ ì¡°ì‘í•˜ëŠ” ë° ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì—¬ê¸°ì„œ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì…ë ¥ì€ \"context\" ë° \"question\" í‚¤ê°€ ìˆëŠ” ë§µì´ ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. ì‚¬ìš©ì ì…ë ¥ì€ ì§ˆë¬¸ì¼ ë¿ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ê³  \"question\" í‚¤ ì•„ë˜ì˜ ì‚¬ìš©ì ì…ë ¥ì„ í†µê³¼ì‹œì¼œì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì¼ë°˜ì ìœ¼ë¡œ ë‘ê°œì˜ Chain ì„ ë¬¶ì–´ì„œ map_Chain ì˜ í˜•íƒœë¡œ ë§ì´ë“¤ ì“´ë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': AIMessage(content=\"Why did the bear break up with his girlfriend? \\n\\nBecause he couldn't bear the distance between them!\", response_metadata={'finish_reason': 'stop', 'logprobs': None}),\n",
       " 'poem': AIMessage(content='In the forest deep, the bear roams free,\\nA majestic creature, wild and full of mystery.', response_metadata={'finish_reason': 'stop', 'logprobs': None})}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(api_key=openai_api_key)\n",
    "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
    ")\n",
    "\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "map_chain.invoke({\"topic\": \"bear\"}) #2ê°œì˜ ê²°ê³¼ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = PromptTemplate.from_template(\"{name}ì€ ì–´ë””ì˜ CEO ì¸ê°€ìš”\")\n",
    "prompt2 = PromptTemplate.from_template(\"{name}ì€ ì–´ë–¤ ì—…ì ì´ ìˆë‚˜ìš”?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain_1 = {\"name\": RunnablePassthrough()} | prompt1 | model\n",
    "chain_2 = {\"name\": RunnablePassthrough()} | prompt2 | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_chain = RunnableParallel(a=chain_1, b=chain_2) #Chain ì„ ì—¬ëŸ¬ê°œ ë§Œë“¤ì–´ì„œ ë¬¶ì–´ì¤€ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': AIMessage(content=\"'tim cook'ì€ ë¯¸êµ­ì˜ CEOì´ë©°, ë¯¸êµ­ì˜ ê¸°ìˆ ê¸°ì—…ì¸ ì• í”Œ(Apple)ì˜ CEOì…ë‹ˆë‹¤.\", response_metadata={'finish_reason': 'stop', 'logprobs': None}),\n",
       " 'b': AIMessage(content='\"country\": \"tim cook\"ì€ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. Tim Cookì€ ì• í”Œ(Apple)ì˜ CEOë¡œ ìœ ëª…í•œ ì¸ë¬¼ì´ë©°, í…Œí¬ë†€ë¡œì§€ ì‚°ì—…ì—ì„œì˜ ì—…ì ìœ¼ë¡œëŠ” ì•„ì´í°, ì•„ì´íŒ¨ë“œ, ë§¥ë¶ ë“±ì˜ ì œí’ˆ ê°œë°œê³¼ í˜ì‹ ì ì¸ ê¸°ìˆ  ë°œì „ì— ê¸°ì—¬í•œ ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.', response_metadata={'finish_reason': 'stop', 'logprobs': None})}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_chain.invoke({\"country\":\"tim cook\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "íŒŒì´í”„ë¼ì¸ì—ì„œ ì„ì˜ì˜ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "- ì´ëŸ¬í•œ í•¨ìˆ˜ì— ëŒ€í•œ ëª¨ë“  ì…ë ¥ì€ ë‹¨ì¼ ì¸ìˆ˜ê°€ ë˜ì–´ì•¼ í•œë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì„¸ìš”. ì—¬ëŸ¬ ì¸ìˆ˜ë¥¼ ë°›ëŠ” í•¨ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ë‹¨ì¼ ì…ë ¥ì„ ë°›ì•„ ì—¬ëŸ¬ ì¸ìˆ˜ë¡œ ì••ì¶•ì„ í‘¸ëŠ” ë˜í¼ë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "model = ChatOpenAI(api_key=openai_api_key)\n",
    "\n",
    "chain1 = prompt | model\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"sonny\") | RunnableLambda(length_function),\n",
    "        \"b\": {\"text1\": itemgetter(\"sonny\"), \"text2\": itemgetter(\"kangin\")}\n",
    "        | RunnableLambda(multiple_length_function),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='6 + 66 = 72', response_metadata={'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"sonny\": \"kangin\", \"kangin\": \"lee-kang-in\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Accepting a Runnable Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹¤í–‰ ê°€ëŠ¥í•œ ëŒë‹¤ëŠ” ì„ íƒì ìœ¼ë¡œ ì½œë°±, íƒœê·¸ ë° ê¸°íƒ€ êµ¬ì„± ì •ë³´ë¥¼ ì¤‘ì²©ëœ ì‹¤í–‰ì— ì „ë‹¬í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” RunnableConfigë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "import json\n",
    "\n",
    "\n",
    "def parse_or_fix(text: str, config: RunnableConfig):\n",
    "    fixing_chain = (\n",
    "        ChatPromptTemplate.from_template(\n",
    "            \"Fix the following text:\\n\\n```text\\n{input}\\n```\\nError: {error}\"\n",
    "            \" Don't narrate, just respond with the fixed data.\"\n",
    "        )\n",
    "        | ChatOpenAI(api_key=openai_api_key)\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except Exception as e:\n",
    "            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)\n",
    "    return \"Failed to parse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'soony': 'kangin'}\n",
      "Tokens Used: 67\n",
      "\tPrompt Tokens: 58\n",
      "\tCompletion Tokens: 9\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.000105\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    output = RunnableLambda(parse_or_fix).invoke(\n",
    "        \"{soony: kangin}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}\n",
    "    )\n",
    "    print(output)\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @Chain Decorator \n",
    "\n",
    "ë°ì½”ë ˆì´í„°ëŠ” ê¸°ì¡´ì˜ Classì— ê¸°ëŠ¥ì„ ì¶”ê°€ ì‹œí‚¬ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/kdb/Desktop/RAG_examples/pseudo_lab_2weeks_method.ipynb ì…€ 29\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/pseudo_lab_2weeks_method.ipynb#X40sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     chain2 \u001b[39m=\u001b[39m prompt2 \u001b[39m|\u001b[39m ChatOpenAI() \u001b[39m|\u001b[39m StrOutputParser()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/pseudo_lab_2weeks_method.ipynb#X40sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m chain2\u001b[39m.\u001b[39minvoke({\u001b[39m\"\u001b[39m\u001b[39mjoke\u001b[39m\u001b[39m\"\u001b[39m: parsed_output1})\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/pseudo_lab_2weeks_method.ipynb#X40sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m custom_chain\u001b[39m.\u001b[39;49minvoke(\u001b[39m\"\u001b[39;49m\u001b[39mbears\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py:3669\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3667\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3668\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfunc\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 3669\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[1;32m   3670\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_invoke,\n\u001b[1;32m   3671\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   3672\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_config(config, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc),\n\u001b[1;32m   3673\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3674\u001b[0m     )\n\u001b[1;32m   3675\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3676\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   3677\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3678\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse `ainvoke` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3679\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py:1405\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1401\u001b[0m     context \u001b[39m=\u001b[39m copy_context()\n\u001b[1;32m   1402\u001b[0m     context\u001b[39m.\u001b[39mrun(var_child_runnable_config\u001b[39m.\u001b[39mset, child_config)\n\u001b[1;32m   1403\u001b[0m     output \u001b[39m=\u001b[39m cast(\n\u001b[1;32m   1404\u001b[0m         Output,\n\u001b[0;32m-> 1405\u001b[0m         context\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m   1406\u001b[0m             call_func_with_variable_args,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1407\u001b[0m             func,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1408\u001b[0m             \u001b[39minput\u001b[39;49m,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1409\u001b[0m             config,\n\u001b[1;32m   1410\u001b[0m             run_manager,\n\u001b[1;32m   1411\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1412\u001b[0m         ),\n\u001b[1;32m   1413\u001b[0m     )\n\u001b[1;32m   1414\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1415\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py:3543\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3541\u001b[0m                 output \u001b[39m=\u001b[39m chunk\n\u001b[1;32m   3542\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3543\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[1;32m   3544\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, \u001b[39minput\u001b[39;49m, config, run_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m   3545\u001b[0m     )\n\u001b[1;32m   3546\u001b[0m \u001b[39m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3547\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/Users/kdb/Desktop/RAG_examples/pseudo_lab_2weeks_method.ipynb ì…€ 29\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/pseudo_lab_2weeks_method.ipynb#X40sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m output1 \u001b[39m=\u001b[39m ChatOpenAI(api_key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msk-POJqCq0nD2WqndNvQxmWT3BlbkFJ8OjWO2AiOiZKxnf2jY3u\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minvoke(prompt_val1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/pseudo_lab_2weeks_method.ipynb#X40sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m parsed_output1 \u001b[39m=\u001b[39m StrOutputParser()\u001b[39m.\u001b[39minvoke(output1)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/pseudo_lab_2weeks_method.ipynb#X40sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m chain2 \u001b[39m=\u001b[39m prompt2 \u001b[39m|\u001b[39m ChatOpenAI() \u001b[39m|\u001b[39m StrOutputParser()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kdb/Desktop/RAG_examples/pseudo_lab_2weeks_method.ipynb#X40sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m chain2\u001b[39m.\u001b[39minvoke({\u001b[39m\"\u001b[39m\u001b[39mjoke\u001b[39m\u001b[39m\"\u001b[39m: parsed_output1})\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")\n",
    "\n",
    "@chain\n",
    "def custom_chain(text):\n",
    "    prompt_val1 = prompt1.invoke({\"topic\": text})\n",
    "    output1 = ChatOpenAI().invoke(prompt_val1)\n",
    "    parsed_output1 = StrOutputParser().invoke(output1)\n",
    "    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()\n",
    "    return chain2.invoke({\"joke\": parsed_output1})\n",
    "\n",
    "custom_chain.invoke(\"bears\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

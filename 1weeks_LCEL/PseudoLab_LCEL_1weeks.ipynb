{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], template='{country}ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ì˜ˆì œ\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{country}ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_api_key = 'your_api_key'\n",
    "\n",
    "model = ChatOpenAI(model = 'gpt-3.5-turbo-0125',openai_api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ’ LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"country\":\"ëŒ€í•œë¯¼êµ­\"}) #invoke í”„ë¡¬í”„íŠ¸ì™€, ëª¨ë¸ì„ ì‚¬ìŠ¬ë¡œ ì—®ì–´ì„œ invoke í•´ì¤€ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected mapping type as input to PromptTemplate. Received <class 'str'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb ì…€ 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m chain\u001b[39m.\u001b[39;49minvoke(\u001b[39m\"\u001b[39;49m\u001b[39më¯¸êµ­\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#  keyì™€ valueë¥¼ ë§¤ì¹­ì´ë˜ëŠ” í˜•íƒœì¸ë° í˜„ì¬ valueë§Œ ë“¤ì–´ì™”ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ ì˜¤ë¥˜ë¥¼ ê°œì„ í•´ì£¼ëŠ” ë°©ë²•\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py:2053\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2051\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2052\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 2053\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   2054\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   2055\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2056\u001b[0m             patch_config(\n\u001b[1;32m   2057\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   2058\u001b[0m             ),\n\u001b[1;32m   2059\u001b[0m         )\n\u001b[1;32m   2060\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   2061\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/prompts/base.py:112\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtags:\n\u001b[1;32m    111\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39mtags\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtags)\n\u001b[0;32m--> 112\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[1;32m    113\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_format_prompt_with_error_handling,\n\u001b[1;32m    114\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    115\u001b[0m     config,\n\u001b[1;32m    116\u001b[0m     run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    117\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/base.py:1246\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m     context \u001b[39m=\u001b[39m copy_context()\n\u001b[1;32m   1243\u001b[0m     context\u001b[39m.\u001b[39mrun(var_child_runnable_config\u001b[39m.\u001b[39mset, child_config)\n\u001b[1;32m   1244\u001b[0m     output \u001b[39m=\u001b[39m cast(\n\u001b[1;32m   1245\u001b[0m         Output,\n\u001b[0;32m-> 1246\u001b[0m         context\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m   1247\u001b[0m             call_func_with_variable_args,\n\u001b[1;32m   1248\u001b[0m             func,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1249\u001b[0m             \u001b[39minput\u001b[39;49m,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1250\u001b[0m             config,\n\u001b[1;32m   1251\u001b[0m             run_manager,\n\u001b[1;32m   1252\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1253\u001b[0m         ),\n\u001b[1;32m   1254\u001b[0m     )\n\u001b[1;32m   1255\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1256\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/kdb/lib/python3.9/site-packages/langchain_core/prompts/base.py:91\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_format_prompt_with_error_handling\u001b[39m(\u001b[39mself\u001b[39m, inner_input: Dict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[1;32m     90\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(inner_input, \u001b[39mdict\u001b[39m):\n\u001b[0;32m---> 91\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m     92\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected mapping type as input to \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(inner_input)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         )\n\u001b[1;32m     95\u001b[0m     missing \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables)\u001b[39m.\u001b[39mdifference(inner_input)\n\u001b[1;32m     96\u001b[0m     \u001b[39mif\u001b[39;00m missing:\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected mapping type as input to PromptTemplate. Received <class 'str'>."
     ]
    }
   ],
   "source": [
    "chain.invoke(\"ë¯¸êµ­\")\n",
    "#  keyì™€ valueë¥¼ ë§¤ì¹­ì´ë˜ëŠ” í˜•íƒœì¸ë° í˜„ì¬ valueë§Œ ë“¤ì–´ì™”ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ ì˜¤ë¥˜ë¥¼ ê°œì„ í•´ì£¼ëŠ” ë°©ë²•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ë¯¸ì–€ë§ˆì˜ ìˆ˜ë„ëŠ” ë„¤í”¼ë„ì…ë‹ˆë‹¤.')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_2 = {\"country\": RunnablePassthrough()} | prompt | model\n",
    "chain_2.invoke(\"ë¯¸ì–€ë§ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb ì…€ 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#RAGì—ì„œì˜ ì˜ˆì œ\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m Chain \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m:retriver, \u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m: RunnablePassthrough()} \u001b[39m|\u001b[39m prompt \u001b[39m|\u001b[39m model\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{retriever}\u001b[39;00m\u001b[39mì—ì„œ \u001b[39m\u001b[39m{question}\u001b[39;00m\u001b[39mì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì•„ì£¼ì„¸ìš”.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#ì´ëŸ¬í•œ í˜•íƒœë¡œ RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•´ì„œ ë¦¬íŠ¸ë¦¬ë²„ì™€ ëª¨ë¸ì„ ì—°ê²°í•´ì¤„ ìˆ˜ ìˆë‹¤.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriver' is not defined"
     ]
    }
   ],
   "source": [
    "#RAGì—ì„œì˜ ì˜ˆì œ\n",
    "Chain = {\"document\":retriver, \"question\": RunnablePassthrough()} | prompt | model\n",
    "\n",
    "prompt = \"{retriever}ì—ì„œ {question}ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì•„ì£¼ì„¸ìš”.\"\n",
    "\n",
    "#ì´ëŸ¬í•œ í˜•íƒœë¡œ RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•´ì„œ ë¦¬íŠ¸ë¦¬ë²„ì™€ ëª¨ë¸ì„ ì—°ê²°í•´ì¤„ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableParallel  \n",
    "ì½”ë“œë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•´ì¤„ë•Œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = PromptTemplate.from_template(\"{country}ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\")\n",
    "prompt2 = PromptTemplate.from_template(\"{country}ì˜ ì¸êµ¬ëŠ” ëª‡ëª…ì´ì•¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_1 = {\"country\": RunnablePassthrough()} | prompt1 | model\n",
    "chain_2 = {\"country\": RunnablePassthrough()} | prompt2 | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_chain = RunnableParallel(a=chain_1, b=chain_2) #Chain ì„ ì—¬ëŸ¬ê°œ ë§Œë“¤ì–´ì„œ ë¬¶ì–´ì¤€ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': AIMessage(content='ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.'),\n",
       " 'b': AIMessage(content='ëŒ€í•œë¯¼êµ­ì˜ ì¸êµ¬ëŠ” ì•½ 51,780ë§Œ ëª…ì…ë‹ˆë‹¤.')}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_chain.invoke({\"country\":\"ëŒ€í•œë¯¼êµ­\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableLambda\n",
    "ë‚´ê°€ ë§Œë“  í•¨ìˆ˜ë¥¼ ë„£ì–´ì„œ íŒŒì´í”„ë¼ì¸ì„ ì—°ê²°í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(text):\n",
    "    return text['a'].content + ' ' +text['b'].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = map_chain | {\"info\": RunnableLambda(combine_text)} | PromptTemplate.from_template(\n",
    "    \"{info} ì˜ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ êµì •í•´ì£¼ê³  ì´ëª¨í‹°ì½˜ì„ ë„£ì–´ì„œ ì™„ì„±ë„ ìˆê²Œ ì¶œë ¥í•˜ì„¸ìš”\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='ì„œìš¸ì…ë‹ˆë‹¤. ëŒ€í•œë¯¼êµ­ì˜ ì¸êµ¬ëŠ” ì•½ 5ì²œë§Œ ëª… ì •ë„ì…ë‹ˆë‹¤. ì˜ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ êµì •í•´ì£¼ê³  ì´ëª¨í‹°ì½˜ì„ ë„£ì–´ì„œ ì™„ì„±ë„ ìˆê²Œ ì¶œë ¥í•˜ì„¸ìš”')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain.invoke({\"country\":\"ëŒ€í•œë¯¼êµ­\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì›Œì‹±í„´ D.C.ì…ë‹ˆë‹¤. 2021ë…„ ê¸°ì¤€ìœ¼ë¡œ ë¯¸êµ­ì˜ ì¸êµ¬ëŠ” ì•½ 3ì–µ 3ì²œë§Œ ëª… ì •ë„ì…ë‹ˆë‹¤. ğŸ™‚')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = map_chain | {\"info\": RunnableLambda(combine_text)} | PromptTemplate.from_template(\n",
    "    \"{info} ì˜ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ êµì •í•´ì£¼ê³  ì´ëª¨í‹°ì½˜ì„ ë„£ì–´ì„œ ì™„ì„±ë„ ìˆê²Œ ì¶œë ¥í•˜ì„¸ìš”\") | model\n",
    "final_chain.invoke(\"ë¯¸êµ­\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parser\n",
    "ëª¨ë¸ì˜ ì¶œë ¥ì„ ì²˜ë¦¬í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bibimbap', 'Kimchi', 'Bulgogi', 'Japchae', 'Tteokbokki']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0,openai_api_key=openai_api_key)\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"subject\": \"popular Korean cusine\"})\n",
    "# CommaSeparatedListOutputParserë¥¼ ì‚¬ìš©í•´ì„œ , ë¥¼ ì‚¬ìš©í•´ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of a cusine\", \"type\": \"string\"}, \"recipe\": {\"title\": \"Recipe\", \"description\": \"recipe to cook the cusine\", \"type\": \"string\"}}, \"required\": [\"name\", \"recipe\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# ìë£Œêµ¬ì¡° ì •ì˜ (pydantic)\n",
    "class CusineRecipe(BaseModel):\n",
    "    name: str = Field(description=\"name of a cusine\")\n",
    "    recipe: str = Field(description=\"recipe to cook the cusine\")\n",
    "\n",
    "# ì¶œë ¥ íŒŒì„œ ì •ì˜\n",
    "output_parser = JsonOutputParser(pydantic_object=CusineRecipe)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the output schema:\n",
    "```\n",
    "{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of a cusine\", \"type\": \"string\"}, \"recipe\": {\"title\": \"Recipe\", \"description\": \"recipe to cook the cusine\", \"type\": \"string\"}}, \"required\": [\"name\", \"recipe\"]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of a cusine\", \"type\": \"string\"}, \"recipe\": {\"title\": \"Recipe\", \"description\": \"recipe to cook the cusine\", \"type\": \"string\"}}, \"required\": [\"name\", \"recipe\"]}\\n```'} template='Answer the user query.\\n{format_instructions}\\n{query}\\n'\n"
     ]
    }
   ],
   "source": [
    "# prompt êµ¬ì„±\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Bibimbap',\n",
       " 'recipe': 'To cook Bibimbap, start by preparing steamed rice. Then, sautÃ© various vegetables such as spinach, carrots, bean sprouts, and mushrooms separately. Cook marinated beef and fry eggs sunny-side up. Place the rice in a bowl, arrange the vegetables and beef on top, and add a fried egg. Serve with spicy gochujang sauce and mix everything together before eating.'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"query\": \"Let me know how to cook Bibimbap\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream\n",
    "Chatgptì™€ Gemini ë¥¼ ë³´ë©´ í™•ì—°íˆ ì°¨ì´ê°€ ìˆë‹¤.  \n",
    "âŒ¨ï¸ Stream ì€ Chatgpt ì²˜ëŸ¼ í† í°ë‹¨ìœ„ì˜ ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆê²Œ ë” (íƒíƒ íƒ€ë‹¤ë‹¥ ë§ˆì¹˜ íƒ€ì ì¹˜ë“¯ì´)\n",
    "- ChatGPT : í† í°ë‹¨ìœ„ì˜ ì²˜ë¦¬ í›„ Print  \n",
    "- Gemini : ëª¨ë“  í† í°ì„ ì²˜ë¦¬ í›„ Print  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream truck break down?\\nIt had too many \"scoops\" on board!'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "#invoke\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\",openai_api_key=openai_api_key)\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the ice cream break up with the waffle cone? It couldn't handle the rocky road!"
     ]
    }
   ],
   "source": [
    "#stream\n",
    "for chunk in chain.stream(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "#í† í° ë‹¨ìœ„ë¡œ ì¶œë ¥\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the ice cream truck break down? It had too many \"scoops\"!"
     ]
    }
   ],
   "source": [
    "#tip time ì„ í†µí•´ì„œ ì¶œë ¥ì„ ì§€ì—°ì‹œì¼œì„œ ëŠë¦¬ê²Œë”ë„ ì¶œë ¥ì´ ê°€ëŠ¥í•˜ë‹¤.\n",
    "import time\n",
    "for chunk in chain.stream(\"ice cream\"):\n",
    "    time.sleep(1)\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch\n",
    "Input_variable ì— ë‹¤ì–‘í•œ ê°’ì„ ë„£ê³  ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í•˜ê³  ì‹¶ì„ë•Œ ì“¸ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why did the ice cream truck break down?\\n\\nBecause it had too many \"scoops\"!',\n",
       " 'Why did the spaghetti go to the party? Because it heard it was going to be a pasta-tively good time!',\n",
       " 'Why did the dumpling go to the party? Because it was on a roll!']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async\n",
    "ë¹„ë™ê¸° ì²˜ë¦¬ ë²„ì „ì´ í•„ìš”í•œ ê²½ìš° ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nasync_client = openai.AsyncOpenAI()\\n\\nasync def acall_chat_model(messages: List[dict]) -> str:\\n    response = await async_client.chat.completions.create(\\n        model=\"gpt-3.5-turbo\", \\n        messages=messages,\\n    )\\n    return response.choices[0].message.content\\n\\nasync def ainvoke_chain(topic: str) -> str:\\n    prompt_value = prompt_template.format(topic=topic)\\n    messages = [{\"role\": \"user\", \"content\": prompt_value}]\\n    return await acall_chat_model(messages)\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LCEL ì´ ì•„ë‹Œê²½ìš°\n",
    "'''\n",
    "async_client = openai.AsyncOpenAI()\n",
    "\n",
    "async def acall_chat_model(messages: List[dict]) -> str:\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def ainvoke_chain(topic: str) -> str:\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n",
    "    return await acall_chat_model(messages)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/56xjd1v51xq5ybs0jdmq217h0000gn/T/ipykernel_10576/1377747015.py:1: RuntimeWarning: coroutine 'RunnableSequence.ainvoke' was never awaited\n",
      "  chain.ainvoke(\"ice cream\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream truck break down? \\n\\nIt had too many \"scoops\"!'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.ainvoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM instead of chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the ice cream go to therapy?\\n\\nBecause it had a rocky road!'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\",openai_api_key=openai_api_key)\n",
    "llm_chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "llm_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different model provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "#openai ëŒ€ì‹ ì— anthropicì„ ì‚¬ìš©í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n",
    "#openai ëª¨ë¸ í˜¸ì¶œê³¼ ë°©ë²•ì€ ë™ì¼í•˜ê¸° ë•Œë¬¸ì—, ëª¨ë¸ë§Œ ë°”ê¿”ì„œ ì„ ì–¸í•´ì£¼ë©´ ë˜ê¸° ë•Œë¬¸ì— í¸ë¦¬í•˜ë‹¤.\n",
    "anthropic = ChatAnthropic(model=\"claude-2\")\n",
    "anthropic_chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | anthropic\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "anthropic_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime configurability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anthropic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb ì…€ 43\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrunnables\u001b[39;00m \u001b[39mimport\u001b[39;00m ConfigurableField\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X66sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m configurable_model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfigurable_alternatives(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X66sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     ConfigurableField(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m), \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X66sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     default_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchat_openai\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X66sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     openai\u001b[39m=\u001b[39mllm,\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X66sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     anthropic\u001b[39m=\u001b[39manthropic,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X66sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X66sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m configurable_chain \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X66sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mtopic\u001b[39m\u001b[39m\"\u001b[39m: RunnablePassthrough()} \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X66sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m|\u001b[39m prompt \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X66sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m|\u001b[39m configurable_model \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X66sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m|\u001b[39m output_parser\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#X66sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'anthropic' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "\n",
    "configurable_model = model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"model\"), \n",
    "    default_key=\"chat_openai\", \n",
    "    openai=llm,\n",
    "    anthropic=anthropic,\n",
    ")\n",
    "configurable_chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | configurable_model \n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurable_chain.invoke(\n",
    "    \"ice cream\", \n",
    "    config={\"model\": \"openai\"} #ëª¨ë¸ì„ openaië¡œ ì„¤ì •\n",
    ")\n",
    "stream = configurable_chain.stream(\n",
    "    \"ice cream\", \n",
    "    config={\"model\": \"anthropic\"} #ëª¨ë¸ì„ anthropicìœ¼ë¡œ ì„¤ì •\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True) # anthropicìœ¼ë¡œ ì¶œë ¥ëœë‹¤.\n",
    "\n",
    "configurable_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"]) #openaië¡œ ì„¤ì •ë˜ì–´ìˆê¸° ë•Œë¬¸ì— openaië¡œ ì¶œë ¥ëœë‹¤.(default)\n",
    "\n",
    "# await configurable_chain.ainvoke(\"ice cream\") ë¹„ë™ê¸°ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "https://smith.langchain.com/public/e4de52f8-bcd9-4732-b950-deee4b04e313/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anthropic_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb ì…€ 46\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#Y103sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mLANGCHAIN_API_KEY\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#Y103sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mLANGCHAIN_TRACING_V2\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kdb/Desktop/Fastcampus/PseudoLab_LCEL_1weeks.ipynb#Y103sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m anthropic_chain\u001b[39m.\u001b[39minvoke(\u001b[39m\"\u001b[39m\u001b[39mice cream\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'anthropic_chain' is not defined"
     ]
    }
   ],
   "source": [
    "#ë­ì²´ì¸ ê³µì‹í™ˆí˜ì´ì§€ë¥¼ ê°€ì…í•´ì„œ LangSmithë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. API keyë¥¼ ë°œê¸‰ë°›ì•„ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "anthropic_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fallbacks\n",
    "ì˜ˆë¹„ ë¡œì§ì„ ì‹¤í–‰ì‹œì¼œì„œ ì‹¤í–‰ í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallback_chain = chain.with_fallbacks([anthropic_chain])\n",
    "\n",
    "fallback_chain.invoke(\"ice cream\")\n",
    "# await fallback_chain.ainvoke(\"ice cream\")\n",
    "fallback_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full code comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, ConfigurableField\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" #langsmithë¥¼ í†µí•´ logging\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"    # prompt template\n",
    ")\n",
    "chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")  # ëª¨ë¸ì„ ì–¸\n",
    "openai = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "anthropic = ChatAnthropic(model=\"claude-2\")\n",
    "model = (\n",
    "    chat_openai\n",
    "    .with_fallbacks([anthropic]) #ì˜ˆë¹„ ëª¨ë¸ì„ ì„¤ì •í•´ì¤€ë‹¤.anthropicìœ¼ë¡œ ì„¤ì •\n",
    "    .configurable_alternatives( \n",
    "        ConfigurableField(id=\"model\"), #ëª¨ë¸ í™˜ê²½ì„¤ì • ì •ì˜ë¥¼ í†µí•´ì„œ openaiëª¨ë¸ ì„¤ì •\n",
    "        default_key=\"chat_openai\",\n",
    "        openai=openai,\n",
    "        anthropic=anthropic,\n",
    "    )\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "#chat_openai ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³ , ì˜ˆë¹„ëª¨ë¸ë¡œ anthropicì„ ì‚¬ìš©í•˜ê³ , ëª¨ë¸í™˜ê²½ì„¤ì •ì„ í†µí•´ì„œ openaiëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Search Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires:\n",
    "# pip install langchain docarray tiktoken\n",
    "\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"where did harrison work?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
